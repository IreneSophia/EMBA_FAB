---
title: "S2: Analysis of FAB eye tracking data with brms"
author: "I. S. Plank"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r settings, include=FALSE}

knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.align = 'center', fig.width = 9)
ls.packages = c("knitr",# kable
    "ggplot2",          # plots
    "brms",             # Bayesian lmms
    "tidyverse",        # tibble stuff
    "ggpubr",           # ggarrange
    "ggrain",           # geom_rain
    "bayesplot",        # plots for posterior predictive checks
    "SBC",              # plots for checking computational faithfulness
    "rstatix",          # anova
    "logspline",        # needed for the Bayesian Spearman correlation
    "BayesFactor", 
    "effectsize",       # allows for interpretation and calculation of effectsizes 
    "bayestestR"        # equivalence_test
)

lapply(ls.packages, library, character.only=TRUE)

# set cores
options(mc.cores = parallel::detectCores())

# set options for SBC
use_cmdstanr = getOption("SBC.vignettes_cmdstanr", TRUE) # Set to false to use rstan instead
options(brms.backend = "cmdstanr")

# using parallel processing
library(future)
plan(multisession)

# Setup caching of results
cache_dir = "./_brms_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

# graph settings 
c_light = "#a9afb2"; c_light_highlight = "#8ea5b2"; c_mid = "#6b98b2" 
c_mid_highlight = "#3585b2"; c_dark = "#0072b2"; c_dark_highlight = "#0058b2" 
c_green = "#009E73"
sz = 1

# custom colour palette
custom.col  = as.character(palette.colors()[2:8])
custom.col2 = c("#E1BE6A", "#40B0A6")

```

<style type="text/css">
.main-container {
  max-width: 1100px;
  margin-left: auto;
  margin-right: auto;
}
</style>

# S2.1 Introduction

This R Markdown script analyses eye tracking data from the FAB (face attention bias) paradigm of the EMBA project. The data was preprocessed before being read into this script. 

## Some general settings

```{r set}

# number of simulations
nsim = 250

# set the seed
set.seed(2468)

```

## Package versions

```{r lib_versions, echo=F}

print(R.Version()$version.string)

for (package in ls.packages) {
  print(sprintf("%s version %s", package, packageVersion(package)))
}

```

## Preparation

First, we load the eye tracking data and combine it with demographic information including the diagnostic status of the subjects. Second, we preprocess the latencies of the saccades and divide them into saccades elicited by the cues and saccades elicited by the target. To do so, we use the knowledge that latencies below 100ms are extremely unlikely and use the global minimum in the density function. We also load in the behavioural data again to be able to use reaction times for further correlational analyses. 

```{r prep, fig.height=4}

# load the data
load("FAB_data.RData")

# combine both behavioural datasets
df.fab = rbind(df.fab, df.exp)
df.fab$diagnosis = factor(df.fab$diagnosis, 
                          levels = c("ADHD", "ASD", "BOTH", "COMP"))

# compute subject specific FAB 
df.fab.agg = df.fab %>%
  group_by(subID, diagnosis, stm, cue) %>%
  # summarise the median reaction time for each stimulus pair
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  ) %>%
  pivot_wider(names_from = cue, values_from = rt.cor) %>%
  # calculate the fab purely based on reaction times
  mutate(
    fab = object - face
  ) %>% group_by(subID, diagnosis) %>%
  # calculate the mean FAB per person
  summarise(
    fab = mean(fab)
  ) %>% ungroup()

# remove participants without any data
df.sac = df.sac %>% filter(!is.na(lat)) %>% ungroup() %>%
  # remove unbelievably short and extremely long saccades
  filter(lat <= quantile(lat, probs = 0.99) &
           lat > 100) %>%
  # recode so that it is more consistent
  mutate(
    direction = if_else(dir_face, "face", "object")
  )

# divide into cue and target saccades
criticalpoints = function(density, threshold = 1){
  up   = sapply(1:threshold, function(n) c(density$y[-(seq(n))], rep(NA, n)))
  down = sapply(-1:-threshold, 
                function(n) c(rep(NA,abs(n)), 
                              density$y[-seq(length(density$y), 
                                             length(density$y) - abs(n) + 1)]))
  a    = cbind(density$y,up,down)
  minima = round(density$x[which(apply(a, 1, min) == a[,1])])
  maxima = round(density$x[which(apply(a, 1, max) == a[,1])])
  return(list(minima = minima, maxima = maxima))
}

points = criticalpoints(density(df.sac$lat))

# get the density of the latencies
dd = with(density(df.sac$lat), data.frame(x,y))

# find which point is the global minimum
lat.points = dd$y[points$minima]
idx = which.min(lat.points)

# print the latency
points$minima[idx]

# plot it all
ggplot(dd, aes(x = x, y = y)) + 
  geom_line() +
  geom_vline(xintercept = points$minima[idx], linetype=3) +
  geom_ribbon(data = subset(dd, x <= points$minima[idx]), 
              aes(ymax = y, fill = "cue-elicited"), ymin = 0, 
              colour = "black", alpha = .8) +
  geom_ribbon(data = subset(dd, x >= points$minima[idx]), 
              aes(ymax = y, fill = "target-elicited"), ymin = 0, 
              colour = "black", alpha = .8) +
scale_fill_manual(name = "test",
values = c("cue-elicited" = custom.col[7], "target-elicited" = custom.col[3]),
labels = c("Cue-elicited", "Target-elicited")) + 
  geom_vline(xintercept = 200) + 
  labs(title = "Classification of saccades", x = "Latency", y = "Density") +
  xlim(0, 800) +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15),
        legend.title=element_blank())

ggsave("Fig2_densLatency.svg", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

The graph above shows the density of the latencies with zero on the x-axis being the onset of the cue. After 200ms, the cue disappears and the target is presented on the screen (solid line). We can see that there is a minimum about 130ms after the target appears (dotted line). We can assume that saccades produced before this were in response to the cue (pink) and saccades after were in response to the target (green). Therefore, we divide the saccades accordingly. Then, we aggregate the data per subject and cue. Last, we set all predictors to sum contrasts.

```{r prepro}

# summarise overall saccade count based on direction: whole trial
df.cnt = df.sac %>% 
  group_by(subID, direction) %>% 
  summarise(
    n.sac = n()
  )

# add a zero if no saccades were produced
subID    = rep(as.character(unique(df.sac$subID)), 
               each = length(unique(df.sac$direction)))
direction = rep(as.character(unique(df.sac$direction)), 
               times = length(unique(df.sac$subID)))
df.cnt = merge(df.cnt, data.frame(subID, direction), all = T) %>%
  mutate(
    n.sac = if_else(is.na(n.sac), 0, n.sac)
  ) %>% 
  # merge with behavioural data
  merge(., df.fab.agg) %>%
  mutate_if(is.character, as.factor)

# code whether or not cue associated saccade occured and capture latencies
df.cue = merge(df.fab, 
               df.sac %>% filter(lat <= points$minima[idx] & sac_trl == 1), 
               all = T) %>%
  select(subID, diagnosis, trl, stm, cue, rt.cor, acc, direction, lat) %>%
  mutate(
    sac = if_else(is.na(direction),0,1)
  ) %>%
  mutate_if(is.character, as.factor)

# preprocess target latencies
df.lat = df.sac %>% 
  # only keep latencies associated with target
  filter(lat > points$minima[idx]) %>%
  # only keep the first target saccade latency of each trial
  group_by(subID, diagnosis, trl, cue) %>%
  filter(sac_trl == min(sac_trl)) %>%
  merge(., df.fab) %>%
  mutate_if(is.character, as.factor)

# set and print the contrasts
contrasts(df.lat$cue) = contr.sum(2)
contrasts(df.lat$cue)
contrasts(df.lat$diagnosis) = contr.sum(4)
contrasts(df.lat$diagnosis)
contrasts(df.cue$direction) = contr.sum(2)
contrasts(df.cue$direction)
contrasts(df.cue$diagnosis) = contr.sum(4)
contrasts(df.cue$diagnosis)
contrasts(df.cnt$direction) = contr.sum(2)
contrasts(df.cnt$direction)
contrasts(df.cnt$diagnosis) = contr.sum(4)
contrasts(df.cnt$diagnosis)

```

# S2.2 Number of saccades towards face during trial

First, we are going to assess the saccades that are produced in the direction of the face throughout the full trial: cue and target presentation. Based on Entzmann et al. (2021), we hypothesised that COMP participants produce more saccades towards the face than towards the object cues during the trials. 

## Specify the model

Since we are counting the number of saccades, we use a poisson distribution for our model. We add an group-level intercept for each subject, and assess the influence of the predictors diagnostic status and whether the saccade was directed towards the side of the face or object as well as the interaction. We set our priors very wide because we do not have strong prior assumptions apart from people producing fewer saccades than there are trials. 

```{r model_cnt}

code = "CNT"

# set the formula
f.cnt = brms::bf(n.sac ~ diagnosis * direction + (1 | subID))

# set priors based on study design
priors = c(
  prior(normal(3,  1.5),    class = Intercept), 
  prior(normal(0,  1.0),    class = sd),
  prior(normal(0,  1.0),    class = b)
)

# set number of iterations and warmup for models
iter = 4500
warm = 1500

```

## Simulation-based calibration

``` {r sbc_cnt}

# check if the SBC already exists
if (file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
  dat        = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
} else {
  # perform the SBC
  set.seed(2468)
  gen = SBC_generator_brms(f.cnt, data = df.cnt, prior = priors, 
   thin = 50, warmup = 10000, refresh = 2000,
   generate_lp = TRUE, family = poisson(), init = 0.1)
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
    warmup = warm, iter = iter)
  dat = generate_datasets(gen, nsim)
  saveRDS(dat, file.path(cache_dir, sprintf("dat_%s.rds", code)))
  res = compute_SBC(dat, 
        bck,
        cache_mode     = "results", 
        cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}
```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and only `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples. This suggests that this model performs well.

## Prior predictive checks

Next, we can plot the simulated values to perform prior predictive checks. 

```{r prior_checks_cnt, fig.height=8}

# get the true values
truePars = dat[["variables"]]

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.cnt)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

# set very large data points to a value of 432
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH > 432] = 432
# compute one histogram per simulated data-set 
breaks = seq(0, max(dvfakematH, na.rm=T), length.out = 100) 
binwidth = round(breaks[2] - breaks[1])
breaks = seq(0, max(dvfakematH, na.rm=T), binwidth) 
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "number of saccades") +
  theme_bw()

tmpM = apply(dvfakematH, 2, mean) # mean 
tmpSD = apply(dvfakematH, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean number of saccades", title = "Means of simulated data") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD number of saccades", title = "Standard deviations of simulated data") +
  theme_bw()

p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, top = text_grob("Prior predictive checks", face = "bold", size = 14))

```

Since our priors were set very wide, we do get wide prior predictive distributions. We accept this and continue with our checks. 

## Computational faithfulness and model sensitivity

```{r comp_check_cnt, fig.height=12}

# get simulation numbers with issues
des_rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = max(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank < des_rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters

df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id)) %>%
  ungroup() %>%
  mutate(
    max_rank = max(rank)
  )
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(
                            as.numeric(
                              gsub(".*, (.+)\\).*", "\\1", 
                                   priors[priors$class == "b",]$prior)),
                            length(unique(df.results.b$variable))-1)), 
                        unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

All of this looks good. Despite our wide priors, the contraction shows a bit of a distribution which increases our trust that the wide priors are appropriate.

## Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_cnt, message=T, fig.height=4}

# fit the model
set.seed(2468)
m.cnt = brm(f.cnt,
            df.cnt, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_cnt-face",
            family = "poisson", 
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.cnt$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.cnt) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.cnt)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_cnt, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.cnt, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.cnt, ndraws = nsim) + 
  theme_bw()

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.cnt$n.sac, post.pred, df.cnt$diagnosis) + 
  theme_bw()

p = ggarrange(p1, p2,
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: number of saccades towards face", 
                face = "bold", size = 14))

```

The predictions based on the model capture the data well. This further increases our trust in the model. 

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_cnt, fig.height=6}

# print a summary
summary(m.cnt)

# get the estimates and compute groups
df.m.cnt = as_draws_df(m.cnt) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    ASD       = b_Intercept + b_diagnosis2,
    ADHD      = b_Intercept + b_diagnosis1,
    BOTH      = b_Intercept + b_diagnosis3,
    COMP      = b_Intercept + b_COMP
    )

# plot the posterior distributions
df.m.cnt %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP"       ~ "COMP",
      "b_direction1" ~ "Face",
      "b_diagnosis1:direction1" ~ "Interaction: ADHD",
      "b_diagnosis2:direction1" ~ "Interaction: ASD",
      "b_diagnosis3:direction1" ~ "Interaction: ADHD+ASD"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# H2a: COMP: face > object
h2a = hypothesis(m.cnt, 
                 "0 < 2*(direction1 - diagnosis1:direction1 - 
                 diagnosis2:direction1 - diagnosis3:direction1)")
h2a

# explore general FAB
e1 = hypothesis(m.cnt, "0 < 2*direction1", 
                alpha = 0.025)
e1

# extract predicted differences
df.new = df.cnt %>% 
  select(diagnosis, direction) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, direction, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.cnt, summary = F, 
               newdata = df.new %>% select(diagnosis, direction), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

vtable::st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

vtable::st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

Our hypothesis regarding the number of saccades towards face or object cues was not confirmed: there was no credible difference in our COMP group (*estimate* = `r round(h2a$hypothesis$Estimate,2)` [`r round(h2a$hypothesis$CI.Lower,2)`, `r round(h2a$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2a$hypothesis$Post.Prob*100,2)`%). Exploration of FAB effect regardless of the group similarly indicates no FAB in the form of a larger number of saccades produced towards the faces over the whole trial (*estimate* = `r round(e1$hypothesis$Estimate,2)` [`r round(e1$hypothesis$CI.Lower,2)`, `r round(e1$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e1$hypothesis$Post.Prob*100,2)`%). 

## Plots

As a next step, we can now finally plot our data. 

```{r plot_cnt, fig.height=4}

# rain cloud plot
df.cnt %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    Direction = recode(direction, "face" = "Face", "object" = "Object")
  ) %>%
  ggplot(aes(diagnosis, n.sac, fill = Direction, colour = Direction)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Number of saccades", x = "", y = "Count") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

ggsave("Fig5_nrSac.svg", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

# S2.3 Latencies of target-elicited saccades

Next, we focus on the latencies of the saccades that are produced during the presentation of the targets to assess whether cue type, diagnostic group or their interaction influence latencies. We hypothesised that ASD participants show an increased latency compared to COMP participants when producing saccades towards targets appearing at the location of a face. 

## Full model

We start with the model containing all latencies of saccades produced during the target presentation. We choose a shifted lognormal distribution because saccade latencies below 100ms are very unlikely. Additionally, latencies are determined based on the onset of the cue presentation (200ms). The SBC was run on three groups. 

### Setting up and assessing the model

```{r model_lat}

code = "LAT"

# set the formula
f.lat = brms::bf(lat ~ diagnosis * cue + (cue | subID) + (diagnosis * cue | stm))

# set weakly informative priors
priors = c(
  prior(normal(5,    0.75), class = Intercept),
  prior(normal(0,    0.25), class = sd),
  prior(normal(0,    0.25), class = b),
  prior(normal(0.5,  0.50), class = sigma),
  prior(normal(350, 50.00), class = ndt),  # threshold between target and cue saccades
  prior(lkj(2),             class = cor)
)

# set number of iterations and warmup for models
iter = 3000
warm = 1000

```

``` {r sbc_lat}

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file = file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # create the data and the results
  set.seed(2468)
  gen = SBC_generator_brms(f.lat, data = df.lat, prior = priors, 
                           family = "shifted_lognormal",
                           thin =  50, warmup = 10000, refresh = 2000,
                           generate_lp = TRUE)
  dat = generate_datasets(gen, nsim) 
  saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  backend = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                            warmup = 1000, iter = 3000)
  results = compute_SBC(dat, backend,
                        cache_mode     = "results", 
                        cache_location = file.path(cache_dir, paste0("res_", code)))
  saveRDS(results$stats, 
          file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(results$backend_diagnostics, 
          file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05 and only `r nrow(df.backend %>% filter(n_divergent > 0))` model had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). This suggests that this model performs well.

### Prior predictive checks

Next, we can plot the simulated values to perform prior predictive checks. 

```{r prior_checks_lat, fig.height=8}

# get the true values
truePars = dat[["variables"]]

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.lat)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

# set very large data points to a value of 1500
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH < 0]    = 0
dvfakematH[dvfakematH > 1500] = 1500
# compute one histogram per simulated data-set 
breaks = seq(0, 1500, length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "latency of saccades") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean latency of saccades", title = "Means of simulated data") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD latency of saccades", title = "Standard deviations of simulated data") +
  theme_bw()

p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, 
                top = text_grob("Prior predictive checks: latency", 
                face = "bold", size = 14))

```

First, we assess whether the simulated values fit our expectations of the distribution of the data. Previous literature has found that saccade latencies are around 200ms with few saccades being produced faster than 100ms. If we add 200ms from the cue presentation, this means we expect most latencies to be above 300ms and centered around 400ms. Our simulated datasets seem to capture this well. 

### Computational faithfulness and model sensitivity

```{r comp_check_lat, fig.height=12}

# get simulation numbers with issues
des_rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = max(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank < des_rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters

df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id)) %>%
  ungroup() %>%
  mutate(
    max_rank = max(rank)
  )
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(
                            as.numeric(
                              gsub(".*, (.+)\\).*", "\\1", 
                                   priors[priors$class == "b",]$prior)),
                            length(unique(df.results.b$variable))-1)), 
                        unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

All looks acceptable here.  

### Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_lat, fig.height=4, message=T}

# fit the maximal model
set.seed(2587)
m.lat = brm(f.lat,
            df.lat, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_lat",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.lat$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.lat) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.lat)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The model does not have any divergent transitions nor high rhats. The trace plots also look good, therefore, we move on to the posterior predictive checks. 

```{r postpc2_lat, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.lat, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.lat, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.lat$lat, post.pred, df.lat$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: latency", 
                face = "bold", size = 14))

```

The simulated data based on the model does not fit our data very well: it is wider and seems to underestimate latencies for COMP while overestimating for ADHD and ASD with the dark blue line showing the mean of the actual dataset and the light blue bars showing the distribution of the predicted data.

## Aggregated model

Since we want to base our inferences on the estimates, we go back to the drawing board and aggregate our data to see whether this resolves these issues. 

### Setting up and assessing the model

```{r model_lat_agg}

code = "LAT_agg"

# aggregate the data
df.lat.agg = df.lat %>% 
  group_by(subID, cue, diagnosis) %>% 
  summarise(lat = median(lat, na.rm = T))

# set the formula
f.lat = brms::bf(lat ~ diagnosis * cue + (1 | subID) )

# set weakly informative priors
priors = priors %>% filter(class != "cor")

# set number of iterations and warmup for models
iter = 3000
warm = 1000

```

Again, we ran the SBC based on the three original, preregistered groups. 

``` {r sbc_lat_agg}

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file = file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # create the data and the results
  set.seed(2468)
  gen = SBC_generator_brms(f.lat, data = df.lat.agg, prior = priors, 
                           family = "shifted_lognormal",
                           thin =  50, warmup = 10000, refresh = 2000,
                           generate_lp = TRUE)
  dat = generate_datasets(gen, nsim) 
  saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  backend = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                            warmup = 1000, iter = 3000)
  results = compute_SBC(dat, backend,
                        cache_mode     = "results", 
                        cache_location = file.path(cache_dir, paste0("res_", code)))
  saveRDS(results$stats, 
          file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(results$backend_diagnostics, 
          file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, but `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). This is something to look out for in the final model. 

### Prior predictive checks

Next, we can plot the simulated values to perform prior predictive checks. 

```{r prior_checks_lat_agg, fig.height=8}

# get the true values
truePars = dat[["variables"]]

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.lat)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

# set very large data points to a value of 1500
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH < 0]    = 0
dvfakematH[dvfakematH > 1500] = 1500
# compute one histogram per simulated data-set 
breaks = seq(0, 1500, length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "latency of saccades") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean latency of saccades", title = "Means of simulated data") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD latency of saccades", title = "Standard deviations of simulated data") +
  theme_bw()

p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, 
                top = text_grob("Prior predictive checks: latency", 
                face = "bold", size = 14))

```

Again, our simulated datasets seem to capture well what we know about saccade latencies.  

### Computational faithfulness and model sensitivity

```{r comp_check_lat_agg, fig.height=12}

# get simulation numbers with issues
des_rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = max(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank < des_rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters

df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id)) %>%
  ungroup() %>%
  mutate(
    max_rank = max(rank)
  )
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(
                            as.numeric(
                              gsub(".*, (.+)\\).*", "\\1", 
                                   priors[priors$class == "b",]$prior)),
                            length(unique(df.results.b$variable))-1)), 
                        unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

The intercept looks slightly off here, the model could have a slight tendency to underestimate it.

### Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_lat_agg, fig.height=4, message=T}

# fit the maximal model
set.seed(7799)
m.lat = brm(f.lat,
            df.lat.agg, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_lat_agg",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.lat$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.lat) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.lat)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The final model does not exhibit any divergence issues or suboptimal rhats. 

```{r postpc2_lat_agg, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.lat, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.lat, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.lat.agg$lat, post.pred, df.lat.agg$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: latency", 
                face = "bold", size = 14))

```

This looks much better with the simulated data based on the model capturing our actual data well. 

### Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_lat, fig.height=4}

# print a summary
summary(m.lat)

# plot the posterior distributions
as_draws_df(m.lat) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
    ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_cue1" ~ "Face",
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP" ~ "COMP",
      "b_diagnosis1:cue1" ~ "Interaction: ADHD x cue",
      "b_diagnosis2:cue1" ~ "Interaction: ASD x cue",
      "b_diagnosis3:cue1" ~ "Interaction: ADHD+ASD x cue"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# H2b: ASD(face) > COMP(face)
h2b = hypothesis(m.lat, 
                 "0 < diagnosis1 + diagnosis3 + 2*diagnosis2 +
                 diagnosis1:cue1 + diagnosis3:cue1 + 2*diagnosis2:cue1")
h2b

# explore: overall faster towards face-cued targets
e = hypothesis(m.lat, "0 > 2*cue1", alpha = 0.025)
e

# extract predicted differences 
df.new = df.lat %>% 
  select(diagnosis, cue) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, cue, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.lat, summary = F, 
               newdata = df.new %>% select(diagnosis, cue), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

vtable::st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    e  = rowMeans(select(., matches(".*_object")), na.rm = T) - 
      rowMeans(select(., matches(".*_face")), na.rm = T)
  )

vtable::st(df.ms %>% 
  mutate(
    # get the face and object latencies -200, so they start with target onset
    face   = rowMeans(select(., matches(".*_face")), na.rm = T) - 200,
    object = rowMeans(select(., matches(".*_object")), na.rm = T) - 200,
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

Our hypothesis that target-elicited saccades towards the faces have a longer latency in ASD than COMP adults was not confirmed by the data (*estimate* = `r round(h2b$hypothesis$Estimate,2)` [`r round(h2b$hypothesis$CI.Lower,2)`, `r round(h2b$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2b$hypothesis$Post.Prob*100,2)`%). Our exploration revealed that that latencies of target-elicited saccades where faster in response to face-cued compared to object-cued targets regardless of the diagnostic group (*estimate* = `r round(e$hypothesis$Estimate,2)` [`r round(e$hypothesis$CI.Lower,2)`, `r round(e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e$hypothesis$Post.Prob*100,2)`%). Specifically, the model predicted a `r round(mean(df.ms$e), 2)`ms [`r round(ci(df.ms$e)$CI_low, 2)`, `r round(ci(df.ms$e)$CI_high, 2)`] shorter latency of for saccades produced towards a face-cued compared to an object-cued target. 

### Plots

```{r plot_lat, fig.height=4}

# rain cloud plot for the 
df.lat.agg %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    Target    = recode(cue, "face" = "Face-cued", "object" = "Object-cued")
  ) %>% 
  ggplot(aes(diagnosis, lat, fill = Target, colour = Target)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Latencies of target-elicited saccades", x = "", y = "Latency (ms)") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

ggsave("Fig6_latSac.svg", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

# S2.4 Correlation with reaction times: number of saccades

Last, we hypothesised that the FAB effect on reaction times may be associated with saccades produced towards the face. To investigate this, we use a Bayesian Spearman correlation as both FAB effect and number of saccades are not normally distributed. 

```{r cor_n, fig.height=4}

# only keep saccades towards faces
df.diff = df.cnt %>% filter(direction == "face")

# check the distribution plot > not normally distributed
p1 = ggplot(df.diff, aes(sample = n.sac)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
p2 = ggplot(df.diff, aes(sample = fab)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
ggarrange(p1, p2, 
          nrow = 1, ncol = 2, labels = "AUTO")

# do a Bayesian Spearman correlation: https://osf.io/j5wud
source("./helpers/rankBasedCommonFunctions.R")
source("./helpers/spearmanSampler.R")

# Default beta prior width is set to a = b = 1 for the sampler 
if (file.exists("rho_CNT.rds")) {
  rhoSamples.cnt = readRDS("rho_CNT.rds")
} else {
  set.seed(1597)
  rhoSamples.cnt = 
    spearmanGibbsSampler(xVals = df.diff$n.sac,
                         yVals = df.diff$fab, 
                         nSamples = 5e3)
  saveRDS(rhoSamples.cnt, file = "rho_CNT.rds")
}

# give the posterior samples for rho to the function below to compute BF01
cor.bf = computeBayesFactorOneZero(rhoSamples.cnt$rhoSamples, 
                          whichTest = "Spearman",
                          priorParameter = 1)

# visualise it
ggplot(data = df.diff, aes(x = n.sac, y = fab)) +
  geom_point(colour = c_mid_highlight) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              geom = "smooth", colour = c_dark_highlight) +
  theme_bw() 

```

Furthermore, we assessed the relationship between FAB and number of saccades produced towards the face on the participant level (see supplementary materials S2.4). We used a Bayesian Spearman correlation due to both values not being normally distributed. This model revealed no association between number of saccades and face attention bias, in fact there was `r interpret_bf(cor.bf)` an association between the number of saccades and face attention bias (log(*BF*) = `r round(log(cor.bf), 2)`).

# S2.5 Exploration: cue-elicited saccades

Additionally to our hypotheses, we also explored any effects of diagnostic status, cue type and their interaction on whether a saccade was produced during the presentation of the cues to assess the findings of increased saccade frequency towards faces by Pereira et al. (2020)in our data. Since these are again count data, we will use a Poisson and then compare the overall descriptives and effects to Pereira and colleague's results. Pereira found about 6% of trials contained cue-elicited saccades which translates to an intercept of 3. Therefore, we can use the same priors and SBC as in our Poisson investigating numbers of saccades in general. 

```{r model_cnt_cue, fig.height=12}

# aggregate to counts
df.cnt.cue = df.cue %>% 
  group_by(subID, diagnosis) %>%
  summarise(
    face   = sum(direction == "face", na.rm = T),
    object = sum(direction == "object", na.rm = T)
  ) %>%
  pivot_longer(cols = c(face, object), names_to = "direction", values_to = "n.sac") %>%
  mutate_if(is.character, as.factor)

# aggregate for descriptives over both conditions
df.cnt.cue.agg = df.cnt.cue %>% group_by(subID, diagnosis) %>%
  summarise(n.sac = sum(n.sac))

# set the contrasts
contrasts(df.cnt.cue$direction) = contr.sum(2)
contrasts(df.cnt.cue$direction)
contrasts(df.cnt.cue$diagnosis) = contr.sum(4)
contrasts(df.cnt.cue$diagnosis)

# set the same formula
f.cnt = brms::bf(n.sac ~ diagnosis * direction + (1 | subID))

# set priors based on study design
priors = c(
  prior(normal(3,  1.5),    class = Intercept), 
  prior(normal(0,  1.0),    class = sd),
  prior(normal(0,  1.0),    class = b)
)

# set number of iterations and warmup for models
iter = 4500
warm = 1500

```

## Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_cnt_cue, message=T, fig.height=4}

# fit the model
set.seed(4682)
m.cnt = brm(f.cnt,
            df.cnt.cue, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_cnt-cue",
            family = "poisson", 
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.cnt$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.cnt) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.cnt)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_cnt_cue, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.cnt, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.cnt, ndraws = nsim) + 
  theme_bw()

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.cnt.cue$n.sac, post.pred, df.cnt.cue$diagnosis) + 
  theme_bw()

p = ggarrange(p1, p2,
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: number of cue-elicited saccades", 
                face = "bold", size = 14))

```

The predictions based on the model capture the data well. This further increases our trust in the model. 

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_cnt_cue, fig.height=6}

# print a summary
summary(m.cnt)

# get the estimates and compute groups
df.m.cnt = as_draws_df(m.cnt) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    ASD       = b_Intercept + b_diagnosis2,
    ADHD      = b_Intercept + b_diagnosis1,
    BOTH      = b_Intercept + b_diagnosis3,
    COMP      = b_Intercept + b_COMP
    )

# plot the posterior distributions
df.m.cnt %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP"       ~ "COMP",
      "b_direction1" ~ "Face",
      "b_diagnosis1:direction1" ~ "Interaction: ADHD",
      "b_diagnosis2:direction1" ~ "Interaction: ASD",
      "b_diagnosis3:direction1" ~ "Interaction: ADHD+ASD"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# face > object
e = hypothesis(m.cnt, "0 < 2*(direction1)", 
                alpha = 0.025)
e


# extract predicted differences
df.new = df.cnt.cue %>% ungroup() %>%
  select(diagnosis, direction) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, direction, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.cnt, summary = F, 
               newdata = df.new %>% select(diagnosis, direction), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

vtable::st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

vtable::st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

On average, participants produced cue-elicited saccades on `r round(100*mean(df.cnt.cue.agg$n.sac)/432, 2)`% +- `r round(sd(100*df.cnt.cue.agg$n.sac/432)/sqrt(nrow(df.cnt.cue.agg)), 2)` of the trials. However, the range was very wide, with some participants producing none and others producing them on `r round(100*max(df.cnt.cue.agg$n.sac)/432, 2)`% of the trials. Regardless of group, credibly more cue-elicited saccades were produced towards face compared to object cues (*estimate* = `r round(e$hypothesis$Estimate,2)` [`r round(e$hypothesis$CI.Lower,2)`, `r round(e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e$hypothesis$Post.Prob*100,2)`%).

## Plots

As a last step, we can plot our data. 

```{r plot_cnt_cue, fig.height=4}

# rain cloud plot
df.cnt %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD")
  ) %>%
  ggplot(aes(diagnosis, n.sac, fill = direction, colour = direction)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Number of cue-elicited saccades", x = "", y = "n") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```

# S2.6 Exploration: Latencies of cue-elicited saccades

## Model

We assume that the SBC for the hypothesis-guided latency analysis holds for this. We only need to slightly adjust the prior for the shift. This analysis only includes participants who performed cue-elicited saccades. 

```{r model_lat_cue}

# preprocess cue latencies
df.lat.cue = df.cue %>%
  filter(!is.na(direction)) %>%
  group_by(subID, direction, diagnosis) %>% 
  summarise(lat = median(lat, na.rm = T)) %>%
  mutate_if(is.character, as.factor) 

# set the formula
f.lat = brms::bf(lat ~ diagnosis * direction + (1 | subID) )

# set weakly informative priors
priors = c(
  prior(normal(5,    0.75), class = Intercept),
  prior(normal(0,    0.25), class = sd),
  prior(normal(0,    0.25), class = b),
  prior(normal(0.5,  0.50), class = sigma),
  prior(normal(150, 50.00), class = ndt)  # this is the only prior that differs
)

# set number of iterations and warmup for models
iter = 3000
warm = 1000

# set the contrasts
contrasts(df.lat.cue$direction) = contr.sum(2)
contrasts(df.lat.cue$direction)
contrasts(df.lat.cue$diagnosis) = contr.sum(4)
contrasts(df.lat.cue$diagnosis)

```

```{r postpc_lat_cue, fig.height=4, message=T}

# fit the maximal model
set.seed(7799)
m.lat = brm(f.lat,
            df.lat.cue, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_lat-cue_agg",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.lat$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.lat) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.lat)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The final model does not exhibit any divergence issues or suboptimal rhats. 

```{r postpc2_lat_cue, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.lat, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.lat, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.lat.cue$lat, post.pred, df.lat.cue$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: latency cues", 
                face = "bold", size = 14))

```

This looks good.

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_lat_cue, fig.height=4}

# print a summary
summary(m.lat)

# plot the posterior distributions
as_draws_df(m.lat) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
    ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_direction1" ~ "Face",
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP" ~ "COMP",
      "b_diagnosis1:direction1" ~ "Interaction: ADHD x direction",
      "b_diagnosis2:direction1" ~ "Interaction: ASD x direction",
      "b_diagnosis3:direction1" ~ "Interaction: ADHD+ASD x direction"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# explore: faster towards faces
e = hypothesis(m.lat, "0 > 2*direction1", alpha = 0.025)
e

# extract predicted differences 
df.new = df.lat.cue %>% ungroup() %>% 
  select(diagnosis, direction) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, direction, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.lat, summary = F, 
               newdata = df.new %>% select(diagnosis, direction), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

vtable::st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    e  = rowMeans(select(., matches(".*_object")), na.rm = T) - 
      rowMeans(select(., matches(".*_face")), na.rm = T)
  )

vtable::st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = e
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

A similar effect was found when exploring the latencies of cue-induced saccade, with participants producing saccades towards face cues faster than saccades towards object cues, independent of group (*estimate* = `r round(e$hypothesis$Estimate,2)` [`r round(e$hypothesis$CI.Lower,2)`, `r round(e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e$hypothesis$Post.Prob*100,2)`%). This model predicted that if a cue-induced saccade was produced towards the face, the latency was `r round(mean(df.ms$e), 2)`ms [`r round(ci(df.ms$e)$CI_low, 2)`, `r round(ci(df.ms$e)$CI_high, 2)`] shorter than if a saccade was produced towards the object cue.  

## Plots

```{r plot_lat_cue, fig.height=4}

# rain cloud plot for the 
df.lat.cue %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD")
  ) %>% 
  ggplot(aes(diagnosis, lat, fill = direction, colour = direction)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Latencies of cue-elicited saccades", x = "", y = "ms") +
  theme_bw() + 
  ylim(0, 325) +
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```

# S2.7 Exploration: Dwell times starting within cue-elicited saccade window

## Model

We assume that the SBC from the reaction time lognormal model holds here, although we slightly widen the Intercept because we have less prior knowledge.

```{r model_fix}

# read in the preprocessed data
df.fix = readRDS("FAB_ET_fix.rds")

# set the formula
f.fix = brms::bf(duration ~ diagnosis * ROI * onTar + 
                   (ROI * onTar | subID) + (ROI * onTar | subID))

# set weakly informative priors
priors = c(
  # general priors based on SBV
  prior(normal(6, 0.6),  class = Intercept),
  prior(normal(0, 0.5),  class = sigma),
  prior(normal(0, 0.1),  class = sd),
  prior(lkj(2),          class = cor),
  prior(normal(0,     0.04), class = b),
  # shift
  prior(normal(200,   100), class = ndt)
)

# set number of iterations and warmup for models
iter = 3000
warm = 1000

# set the contrasts
contrasts(df.fix$ROI) = contr.sum(2)
contrasts(df.fix$ROI)
contrasts(df.fix$onTar) = contr.sum(2)
contrasts(df.fix$onTar)
contrasts(df.fix$diagnosis) = contr.sum(4)
contrasts(df.fix$diagnosis)

```

```{r postpc_fix, fig.height=9, message=T}

# fit the maximal model
set.seed(5599)
m.fix = brm(f.fix,
            df.fix, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_fix",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.fix$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.fix) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.fix)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The final model does not exhibit any divergence issues or suboptimal rhats. 

```{r postpc_fix2, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.fix, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.fix, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.fix$duration, post.pred, df.fix$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: dwell times", 
                face = "bold", size = 14))

```

This looks good.

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_fix, fig.height=6}

# print a summary
summary(m.fix)

# plot the posterior distributions
as_draws_df(m.fix) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
    ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "ROI1", "FaceROI"),
    coef = str_replace_all(coef, "onTar1", "notTarget"),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

```

## Plots

```{r plot_fix, fig.height=4}

# rain cloud plot for the 
df.fix %>%
  group_by(subID, diagnosis, ROI, onTar) %>%
  summarise(
    duration = median(duration, na.rm = T)
  ) %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD")
  ) %>% 
  ggplot(aes(diagnosis, duration, fill = ROI, colour = ROI)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Dwell times starting during cue-elicited saccades", x = "", y = "") +
  theme_bw() + 
  facet_wrap(. ~ onTar) +
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```
