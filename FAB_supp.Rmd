---
title: "Supplementary materials"
subtitle: "Comparison of face attention bias in adults with ASD, ADHD or comorbid ADHD+ASD"
author: "I. S. Plank"
date: "`r Sys.Date()`"
output: 
  word_document:
    toc: true
    number_sections: true
fontsize: 10pt
geometry: margin=0.5in
---
  
```{r settings, include=FALSE}

knitr::opts_chunk$set(echo = T, warning = F, message = F, 
                      fig.align = 'center', fig.width = 7.4)
ls.packages = c("knitr",# kable
    "ggplot2",          # plots
    "brms",             # Bayesian lmms
    "designr",          # simLMM
    "bridgesampling",   # bridge_sampler
    "tidyverse",        # tibble stuff
    "ggpubr",           # ggarrange
    "vtable",           # summary table st
    "ggrain",           # geom_rain
    "bayesplot",        # plots for posterior predictive checks
    "SBC",              # plots for checking computational faithfulness
    "rstatix",          # anova
    "flextable",        # regulartable
    "officer",          # read_docx
    "BayesFactor", 
    "effectsize",       # allows for interpretation and calculation of effectsizes 
    "bayestestR"        # equivalence_test
)

lapply(ls.packages, library, character.only=TRUE)

# set cores
options(mc.cores = parallel::detectCores())

# set options for SBC
use_cmdstanr = getOption("SBC.vignettes_cmdstanr", TRUE) # Set to false to use rstan instead
options(brms.backend = "cmdstanr")

# using parallel processing
library(future)
plan(multisession)

# Setup caching of results
cache_dir = "./_brms_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

# graph settings 
c_light = "#a9afb2"; c_light_highlight = "#8ea5b2"; c_mid = "#6b98b2" 
c_mid_highlight = "#3585b2"; c_dark = "#0072b2"; c_dark_highlight = "#0058b2" 
c_green = "#009E73"
sz = 1
a = 0.5

# custom colour palette
custom.col  = as.character(palette.colors()[2:8])
custom.col2 = c("#E1BE6A", "#40B0A6")

# calculate visual angle: px.x should be a vector of c(width, height) in pixel
vis_ang = function(px.x) {
  
  # infos on the setup
  mm.w = 344    # monitor width in mm
  mm.h = 215    # monitor height in mm
  px.w = 2650   # monitor resolution: width
  px.h = 1600   # monitor resolution: height
  dist = 570    # viewing distance
  
  # convert width from screen pixel size to mm and take half
  mm.x    = (px.x[1] / (px.w/mm.w))/2
  
  # convert height from screen pixel size to mm and take half
  mm.x[2] = (px.x[2] / (px.h/mm.h))/2
  
  # calculate angles
  rad.alpha    = atan(mm.x[1]/dist)
  rad.alpha[2] = atan(mm.x[2]/dist)
  
  # convert to degrees and double again
  deg.alpha    = (rad.alpha[1] / (pi/180)) * 2
  deg.alpha[2] = (rad.alpha[2] / (pi/180)) * 2
  
  return(deg.alpha)
  
}


```

<style type="text/css">
  .main-container {
    max-width: 1100px;
    margin-left: auto;
    margin-right: auto;
  }
</style>
  
# Introduction
  
This R Markdown script analyses behavioural data from the FAB (face attention bias) paradigm of the EMBA project. The data was preprocessed before being read into this script. 

The task is modeled after Jakobsen et al. (2021), *Attention, Perception, & Psychophysics* and the authors were kind enough to share their  stimuli. Each trial starts with a black fixation cross on a white background. Then, a cue consisting of a pair of pictures, one object and one face, is shown with one picture on the left and one on the right of the previous location of the fixation cross. In line with Moore et al. (2012), *J Autism Dev Disord*, we set the duration of the cue presentation to 200ms. Afterwards, a target square appears either at the previous location of the face or the object. Subjects task is to determine the location (right or left) of the target as fast and accurate as possible. The target only disappears when the participant gives their answer. 

The visual angle of the target was `r round(vis_ang(c(90,90))[1],2)` degrees, the visual angle of the cues was `r round(vis_ang(c(326,326))[1],2)` and the distance of the centre of the target and cue from the fixation cross was `r round(vis_ang(c(410,410))[1]/2,2)` degrees.

## Some general settings

```{r set}

# number of simulations
nsim = 250

# set number of iterations and warmup for models
iter = 3000
warm = 1000

# set the seed
set.seed(2468)

```

## Package versions

The following packages are used in this RMarkdown file: 
  
```{r lib_versions, echo=F}

print(R.Version()$version.string)

for (package in ls.packages) {
  print(sprintf("%s version %s", package, packageVersion(package)))
}

```

## General info

We planned to determine the group-level effect subjects following Barr (2013). For each model, experiment specific priors were set based on previous literature or the task (see comments in the code).

We performed prior predictive checks as proposed in Schad, Betancourt and Vasishth (2020) using the SBC package based on the original design with three groups. To do so, we create `r nsim` simulated datasets where parameters are simulated from the priors. These parameters are used to create one fake dataset. Both the true underlying parameters and the simulated discrimination values are saved.

Then, we create graphs showing the prior predictive distribution of the simulated discrimination threshold to check whether our priors fit our general expectations about the data. Next, we perform checks of computational faithfulness and model sensitivity as proposed by Schad, Betancourt and Vasishth (2020) and implemented in the SBC package. We create models for each of the simulated datasets. Last, we calculate performance metrics for each of these models, focusing on the population-level parameters. We did not rerun SBC after adding the exploratory sample of ADHD+ASD.

We base our assessment of the hypothesis on the posterior distributions. Therefore, we perform posterior prdictive checks and in some cases simplify the model by aggregating values to improve posterior fit. 

## Preparation and group comparisons

First, we load the data and combine it with demographic information including the diagnostic status of the subjects. Then, all predictors are set to sum contrasts. We have a look at the demographics describing our four diagnostic groups: adults with ADHD, autistic adults, autistic adults with ADHD (explorative) and adults without any neurological and psychiatric diagnoses. 

Since this is sensitive data, we load the anonymised version of the processed data at this point but also leave the code we used to create it. 

```{r prep_data}

# check if the data file exists, if yes load it:
if (!file.exists("FAB_data.RData")) {

  # get demo info for subjects
  df.sub = read_csv(file.path("/home/emba/Documents/EMBA/CentraXX", "EMBA_centraXX.csv"), 
                    show_col_types = F) %>%
    mutate(
      diagnosis = recode(diagnosis, "CTR" = "COMP"),
      adhd.meds.desc = adhd.meds,
      adhd.meds = if_else(is.na(adhd.meds), FALSE, TRUE)
    )
  
  # set the data path
  dt.path  = "/home/emba/Documents/EMBA/BVET"
  dt.explo = "/home/emba/Documents/EMBA/BVET-explo"
  
  # load excluded participants (low accuracy, change in diagnosis)
  exc = c(scan(file.path(dt.path, 'FAB_exc.txt'), what="character", sep=NULL),
          scan(file.path(dt.explo, 'FAB_exc.txt'), what="character", sep=NULL))
  df.exc = df.sub %>% filter(subID %in% exc) %>% 
    select(diagnosis) %>% 
    group_by(diagnosis) %>% count()
  
  # load the behavioral data and merge with group
  df.fab = merge(df.sub %>% select(subID, diagnosis, 
                                   RAADS_total, ASRS_total, adhd.meds, gender), 
                 readRDS(file = paste0(dt.path, '/df_FAB.RDS'))) %>%
    mutate_if(is.character, as.factor) %>%
    filter(!(subID %in% exc))
  df.exp = merge(df.sub %>% select(subID, diagnosis, 
                                   RAADS_total, ASRS_total, adhd.meds, gender), 
                 readRDS(file = paste0(dt.explo, '/df_FAB.RDS'))) %>%
    mutate_if(is.character, as.factor) %>%
    filter(!(subID %in% exc))
  
  # only keep participants included in the study in the subject data frame
  subIDs = as.character(c(unique(df.fab$subID), unique(df.exp$subID)))
  df.sub = df.sub %>% filter(subID %in% subIDs)
  
  df.med = df.sub %>% group_by(diagnosis) %>%
    summarise(
      adhd.meds = mean(adhd.meds)
    )
  
  adhd.meds.desc = unique(df.sub[!is.na(df.sub$adhd.meds.desc),]$adhd.meds.desc)
  
  # load the eye tracking data and only keep participants included in the study,
  # so no people with more than 33% mistakes, no people without any saccades 
  # and no people with too many blinks
  df.sac = rbind(readRDS(file.path(dt.explo, "FAB_ET_data.rds")),
                 readRDS(file.path(dt.path,  "FAB_ET_data.rds"))) %>%
    merge(., df.sub %>% select(subID, diagnosis), keep.y = T)
  
  # check groups of people who had no relevant saccades at all
  df.nosac = df.sac %>% filter(is.na(trl)) %>%
    group_by(diagnosis) %>%
    count()

  # anonymise the data
  df.fab = df.fab %>%
    mutate(
      PID = subID,
      subID = as.numeric(subID)
    )
  df.exp = df.exp %>%
    mutate(
      PID = subID,
      subID = as.factor(as.numeric(subID) + max(df.fab$subID))
    )
  
  # get a correspondence of original PIDs and anonymised subIDs
  df.recode = rbind(df.fab %>% select(PID, subID) %>% distinct(),
                    df.exp %>% select(PID, subID) %>% distinct())
  recode = as.character(df.recode$subID)
  names(recode) = df.recode$PID
  df.fab = df.fab %>% select(-PID)
  df.exp = df.exp %>% select(-PID)
  
  # anonymise ET data in the same way
  df.sac$subID = str_replace_all(df.sac$subID, recode)
  
  # print gender frequencies and compare them across groups
  tb.gen = xtabs(~ gender + diagnosis, data = df.sub)
  ct.full = contingencyTableBF(tb.gen, 
                               sampleType = "indepMulti", 
                               fixedMargin = "cols")
  # since only DAN in the ADHD group, we try again after excluding them
  ct.mf = contingencyTableBF(tb.gen[2:3,], 
                             sampleType = "indepMulti", 
                             fixedMargin = "cols")
  # we add this information to our demographics tablerbind(df.demo, 
  df.demo = data.frame(
                    measurement = "Gender",
                    ADHD         = sprintf("%.0f - %.0f - %.0f", 
                                          tb.gen["fem","ADHD"], 
                                          tb.gen["mal","ADHD"],
                                          tb.gen["dan","ADHD"]
                    ),
                    `ADHD+ASD`         = sprintf("%.0f - %.0f - %.0f", 
                                          tb.gen["fem","BOTH"], 
                                          tb.gen["mal","BOTH"],
                                          tb.gen["dan","BOTH"]
                    ),
                    ASD         = sprintf("%.0f - %.0f - %.0f", 
                                          tb.gen["fem","ASD"], 
                                          tb.gen["mal","ASD"],
                                          tb.gen["dan","ASD"]
                    ),
                    COMP         = sprintf("%.0f - %.0f - %.0f", 
                                          tb.gen["fem","COMP"], 
                                          tb.gen["mal","COMP"],
                                          tb.gen["dan","COMP"]
                    ),
                    bf.log      = ct.full@bayesFactor[["bf"]]
                  )
  
  # then, we save some more gender information in a table in case we need it
  tb.gen = xtabs(~ gender + diagnosis + cis, data = df.sub)
  
  # get the gender descriptions of the not-male and not-female participants
  gen.desc = unique(tolower(df.sub[df.sub$gender == "dan",]$gender_desc))
  
  # convert the measures to long which we include in the participant table
  df.sublng = df.sub %>%
    # rename some of the variables
    rename(
      "RADS-R" = "RAADS_total",
      "ASRS-v1.1" = "ASRS_total",
      "Age"    = "age",
      "IQ estimate" = "iq",
      "Education" = "edu"
    ) %>%
    select(diagnosis, Age, `IQ estimate`, `ASRS-v1.1`, `RADS-R`, Education) %>%
    pivot_longer(cols = where(is.numeric)) %>%
    mutate_if(is.character, as.factor)
  
  # initialise the data frame for posthoc tests
  df.post = data.frame()
  
  # now we loop through our measurements to create our demographics table
  for (m in unique(df.sublng$name)) {
    # select the relevant part of df.sub
    df.rel = df.sublng %>% filter(name == m)
    # check which of the group's data is not normally distributed
    df.sht = df.rel %>% 
      group_by(diagnosis) %>%
      shapiro_test(value) %>%
      filter(p < 0.05)
    # if more than zero is not normally distributed...
    if (nrow(df.sht) > 0) {
      # rank transform the data
      df.rel = df.rel %>% ungroup() %>% mutate(value = rank(value))
    }
    # compute the ANOVA
    aov = anovaBF(value ~ diagnosis, data = df.rel)
    # get back the original, untransformed values 
    df.rel = df.sublng %>% filter(name == m)
    # put all the information into the demographics table
    df.demo = rbind(df.demo, 
                    data.frame(
                      measurement = m,
                      ADHD        = sprintf("%.2f ±%.2f (%.0f to %.0f)", 
                                            # ignore NAs because edu missing for one person
                                            mean(df.rel[df.rel$diagnosis == "ADHD",]$value, na.rm = T), 
                                            sd(df.rel[df.rel$diagnosis == "ADHD",]$value, na.rm = T)/
                                              sqrt(sum(df.rel$diagnosis == "ADHD")), 
                                            min(df.rel[df.rel$diagnosis == "ADHD",]$value, na.rm = T), 
                                            max(df.rel[df.rel$diagnosis == "ADHD",]$value, na.rm = T)
                      ),
                      `ADHD+ASD`  = sprintf("%.2f ±%.2f (%.0f to %.0f)", 
                                            mean(df.rel[df.rel$diagnosis == "BOTH",]$value), 
                                            sd(df.rel[df.rel$diagnosis == "BOTH",]$value)/
                                              sqrt(sum(df.rel$diagnosis == "BOTH")),  
                                            min(df.rel[df.rel$diagnosis == "BOTH",]$value), 
                                            max(df.rel[df.rel$diagnosis == "BOTH",]$value)
                      ),
                      ASD         = sprintf("%.2f ±%.2f (%.0f to %.0f)", 
                                            mean(df.rel[df.rel$diagnosis == "ASD",]$value), 
                                            sd(df.rel[df.rel$diagnosis == "ASD",]$value)/
                                              sqrt(sum(df.rel$diagnosis == "ASD")), 
                                            min(df.rel[df.rel$diagnosis == "ASD",]$value), 
                                            max(df.rel[df.rel$diagnosis == "ASD",]$value)
                      ),
                      COMP        = sprintf("%.2f ±%.2f (%.0f to %.0f)", 
                                            mean(df.rel[df.rel$diagnosis == "COMP",]$value), 
                                            sd(df.rel[df.rel$diagnosis == "COMP",]$value)/
                                              sqrt(sum(df.rel$diagnosis == "COMP")), 
                                            min(df.rel[df.rel$diagnosis == "COMP",]$value), 
                                            max(df.rel[df.rel$diagnosis == "COMP",]$value)
                      ),
                      bf.log      = aov@bayesFactor[["bf"]]
                    ))
    
    # next, we want to check whether there are group differences
    if (abs(exp(aov@bayesFactor$bf)) > 3) {
      # do the group comparisons 
      aov.ADHDvASD  = anovaBF(value ~ diagnosis, 
                              data = df.rel %>% filter(diagnosis %in% c("ADHD", "ASD")))
      aov.ADHDvBOTH = anovaBF(value ~ diagnosis, 
                              data = df.rel %>% filter(diagnosis %in% c("ADHD", "BOTH")))
      aov.ADHDvCOMP = anovaBF(value ~ diagnosis, 
                              data = df.rel %>% filter(diagnosis %in% c("ADHD", "COMP")))
      aov.ASDvBOTH  = anovaBF(value ~ diagnosis, 
                              data = df.rel %>% filter(diagnosis %in% c("ASD",  "BOTH")))
      aov.ASDvCOMP  = anovaBF(value ~ diagnosis, 
                              data = df.rel %>% filter(diagnosis %in% c("ASD",  "COMP")))
      aov.BOTHvCOMP = anovaBF(value ~ diagnosis, 
                              data = df.rel %>% filter(diagnosis %in% c("BOTH", "COMP")))
      # put into the posthoc data frame
      df.post = rbind(df.post, 
                    data.frame(
                      measurement = m,
                      ADHDvASD    = aov.ADHDvASD@bayesFactor[["bf"]],
                      ADHDvBOTH   = aov.ADHDvBOTH@bayesFactor[["bf"]],
                      ADHDvCOMP   = aov.ADHDvCOMP@bayesFactor[["bf"]],
                      ASDvBOTH    = aov.ASDvBOTH@bayesFactor[["bf"]],
                      ASDvCOMP    = aov.ASDvCOMP@bayesFactor[["bf"]],
                      BOTHvCOMP   = aov.BOTHvCOMP@bayesFactor[["bf"]]
                    ))
    }
  }
  
  # save the demographics and the posthoc table as word documents
  read_docx() %>%
    body_add_table(df.demo %>% arrange(measurement) %>% 
                         mutate(bf.log = 
                                  if_else(
                                    bf.log > log(3), 
                                    sprintf("%.3f*", bf.log),
                                    sprintf("%.3f", bf.log)))) %>%
    print(target = "FAB_demo.docx")
  read_docx() %>%
    body_add_table(df.post  %>% 
                     mutate_if(is.numeric, 
                               ~ifelse(.>log(3),sprintf("%.3f*", .),sprintf("%.3f", .)))) %>%
    print(target = "FAB_post.docx")
  
  # check how many of each group are above threshold for asrs and rads
  tb.screen = xtabs(~ diagnosis + screening, 
                   data = df.sub %>%
                     select(diagnosis, ASRS_screen, RAADS_total) %>%
                     mutate(
                       screening = case_when(
                         ASRS_screen >= 4 & RAADS_total >  81 ~ "screenBOTH",
                         ASRS_screen >= 4 & RAADS_total <= 81 ~ "screenADHD",
                         ASRS_screen <  4 & RAADS_total <= 81 ~ "screenNone",
                         ASRS_screen <  4 & RAADS_total >  81 ~ "screenASD"
                       )
                     ))
  
  # save it all
  save(df.fab, df.sac, df.exp, ct.full, ct.mf, df.exc, tb.screen,
       df.nosac, gen.desc, tb.gen, adhd.meds.desc, df.med,
       file = "FAB_data.RData")
  
} else {
  
  load("FAB_data.RData")
  
}

# print the group of excluded participants based on low accuracy (< 2/3)
# as well as change in diagnosis
kable(df.exc)
rm(df.exc)

# print the group of the participants included in behavioural and eye tracking 
kable(merge(
  df.sac %>% filter(!is.na(trl)) %>% select(subID, diagnosis) %>% distinct() %>% 
    group_by(diagnosis) %>% summarise(`sample size eye tracking` = n()),
  rbind(df.fab, df.exp) %>% select(subID, diagnosis) %>% distinct() %>% 
    group_by(diagnosis) %>% summarise(`sample size behavioural` = n())
  ))

# Note: eye-tracking only collected if calibration accuracy < 0.5, then exclusion:
# 1 due to more than 1/3 blinks 
# 2 due to no relevant saccades
# how many have been removed due to no relevant saccades?
kable(df.nosac)
rm(df.nosac)

# print the how many people are above threshold for clinical self assessment
kable(tb.screen)

# print the outcome of the two contingency tables for comparison
# based on full sample with agender, diverse and non-binary in one category
ct.full@bayesFactor
# based on female and male participants only
ct.mf@bayesFactor

# combine explorative and original data
df.fab = rbind(df.fab, df.exp)

# set the levels of the diagnosis factor
df.fab$diagnosis = factor(df.fab$diagnosis, 
                          levels = c("ADHD", "ASD", "BOTH", "COMP"))

# set and print the contrasts
contrasts(df.fab$cue) = contr.sum(2)
contrasts(df.fab$cue)
contrasts(df.fab$diagnosis) = contr.sum(4)
contrasts(df.fab$diagnosis)

```

The three diagnostic groups are similar in age, IQ and gender distribution. However, they seem to differ in their questionnaire scores measuring ADHD (ASRS), depression (BDI), autism (RAADS) and alexithymia (TAS). 

# Reaction times

First, we analyse the reaction times for all correctly answered trials to assess whether participants answer faster if the target appears at the previous location of the face, which we refer to as face attention bias (FAB). In our preregistration, we formulated the following hypotheses: 

H1a) COMP participants react faster in response to targets appearing on the side of the face compared to targets appearing on the side of the object (face attention bias; Jakobsen et al., 2021).
H1b) ADHD participants react slower than COMP participants in both cue conditions (Sonuga-Barke et al., 2004).
H1c) ASD participants react slower than COMP participants in both cue conditions (Ghosn et al., 2018).
H1d) Face attention bias is decreased in ASD participants compared to COMP participants (Moore et al., 2012).
H1e) Face attention bias in ADHD participants differs from face attention bias in COMP participants.

## Full model

### Simulation-based calibration

First, we attempted to use a full model for the data. This model includes multiple instances of each stimulus per participant in each of the conditions (face cue or object cue). Therefore, we need slopes for the cue per subject as well as for cue, diagnosis and their interaction for the stimulus. 

```{r sbc_checks}

code = "FAB"

# full model formula
f.fab = brms::bf(rt.cor ~ diagnosis * cue + (cue | subID) + (cue * diagnosis | stm) )

# set informed priors based on previous results
priors = c(
  # general priors based on SBV
  prior(normal(6, 0.3),  class = Intercept),
  prior(normal(0, 0.5),  class = sigma),
  prior(normal(0, 0.1),  class = sd),
  prior(lkj(2),          class = cor),
  # face attention bias effect based on Jakobsen et al. (2021)
  prior(normal(-0.01, 0.04), class = b, coef = cue1),
  # ADHD subjects being slower based on Pievsky & McGrath (2018)
  prior(normal(0.025, 0.04),   class = b, coef = diagnosis1),
  # ASD subjects being slower based on Morrison et al. (2018)
  prior(normal(0.025, 0.04),   class = b, coef = diagnosis2),
  # decreased FAB in ASD subjects based on Moore et al. (2012)
  prior(normal(0.01,  0.04),   class = b, coef = diagnosis2:cue1),
  # no specific expectations for FAB in ADHD
  prior(normal(0,     0.04), class = b),
  # shift
  prior(normal(200,   100), class = ndt)
)

# check if the SBC already exists
if (file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
  dat        = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
} else {
  # perform the SBC
  gen = SBC_generator_brms(f.fab, data = df.fab, prior = priors,
                           family = shifted_lognormal,
                           thin = 50, warmup = 20000, refresh = 2000)
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                      init = 0.1, warmup = warm, iter = iter)
  dat = generate_datasets(gen, nsim)
  saveRDS(dat, file.path(cache_dir, sprintf("dat_%s.rds", code)))
  res = compute_SBC(dat, 
        bck,
        cache_mode     = "results", 
        cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` model had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). This suggests that this model performs well. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r sbc_checks2, fig.height=8}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.fab)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# set large values to a max
dvfakemat[dvfakemat > 2000] = 2000

# compute one histogram per simulated data-set 
binwidth = 20 
breaks = seq(0, max(dvfakemat, na.rm=T) + binwidth, binwidth) 
histmat = matrix(NA, ncol = length(dat), nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakemat[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Distribution of simulated discriminations", y = "", x = "") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean RTs (ms)", title = "Means of simulated RTs") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD RTs (ms)", title = "SDs of simulated RTs") +
  theme_bw()
p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, 
                top = text_grob("Prior predictive checks: reaction times", 
                face = "bold", size = 14))

```

Subfigure A shows the distribution of the simulated data with bluer bands being more likely than greyer bands. It shows a distribution that fits our expectations about reaction times in a simple decision task. The same applies to the distribution of the means and standard deviations in the simulated datasets. We go ahead with these priors and check the results of the SBC. We only plot the results from the models that had no divergence issues. 

```{r sbc_checks3, fig.height=12}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          as.numeric(
                            gsub(".*, (.+)\\).*", "\\1", 
                                 priors[priors$class == "b",]$prior))), 
                                          unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, top = 
                  text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

Next, we check the ranks of the parameters. If the model is unbiased, these should be uniformly distributed (Schad, Betancourt and Vasishth, 2020). The sample empirical cumulative distribution function (ECDF) lies within the theoretical distribution (95%) and the rank histogram also shows ranks within the 95% expected range, although there are some small deviations. We judge this to be acceptable.

Then, we investigated the relationship between the simulated true parameters and the posterior estimates. Although there are individual values diverging from the expected pattern, most parameters were recovered successfully within an uncertainty interval of alpha = 0.05. 

Last, we explore the z-score and the posterior contraction of our population-level predictors. The z-score "determines the distance of the posterior mean from the true simulating parameter", while the posterior contraction "estimates how much prior uncertainty is reduced in the posterior estimation" (Schad, Betancourt and Vasisth, 2020). All of this looks good for this model. 

### Posterior predictive checks

As the next step, we fit the model to the data, check whether there are divergence or rhat issues, and then check whether the chains have converged.

```{r postpc, fig.height=6, message=T}

# fit the full model
set.seed(2469)
m.fab = brm(f.fab,
            df.fab, prior = priors,
            family = shifted_lognormal,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_fab_full"
            )
rstan::check_hmc_diagnostics(m.fab$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.fab) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.fab)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no pathological behaviour with E-BFMI, no divergent samples and no rhat that is higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2, fig.height=9}

# get posterior predictions
post.pred = posterior_predict(m.fab, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.fab, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none") + xlim(0, 2000)

# get rid of NAs in data frame for plotting
df.fab.na = df.fab[!is.na(df.fab$rt.cor),]

# distributions of means compared to the real values per group
p2 = ppc_stat_grouped(df.fab.na$rt.cor, post.pred, df.fab.na$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means compared to the real values per cue
p3 = ppc_stat_grouped(df.fab.na$rt.cor, post.pred, df.fab.na$cue) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, p3,
          nrow = 3, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: RTs", 
                face = "bold", size = 14))

```

Although the overall shape in subfigure A of the simulated data fits well with the real data, the model seems to underestimate the reaction times of the ADHD and ASD groups and overestimate the reaction times of the COMP group: the dark blue line shows the mean of the actual dataset while the light blue bars show the distribution of the predicted data. 

Since we are interested in accurate estimates, we decide to aggregate with the median of the reaction times per stimulus (face-object cue combination) and cue. Then, there are no missing values in the data and we model an estimate for each specific stimulus and cue combination for each participant. 

## Aggregated model

First, we compute the aggregation and have a quick look at the resulting data. 

``` {r agg_setup}

# keep full dataframe
df.fab.full = df.fab

# aggregate reaction times
df.fab = df.fab %>%
  group_by(subID, diagnosis, stm, cue) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  ) %>% ungroup() %>%
  mutate_if(is.character, as.factor)

# set and print the contrasts
contrasts(df.fab$cue) = contr.sum(2)
contrasts(df.fab$cue)
contrasts(df.fab$diagnosis) = contr.sum(4)
contrasts(df.fab$diagnosis)

summary(df.fab)

```

There are now no NAs in the data, because no one made an error on all instances of one stimulus combination.

### Stimulation-based calibration

We again perform an SBC. The model formula and priors can stay the same.

``` {r agg_sbc_check}

code = "FAB_agg"

# check if the SBC already exists
if (file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
  dat        = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
} else {
  # perform the SBC
  gen = SBC_generator_brms(f.fab, data = df.fab, prior = priors, 
                           family = shifted_lognormal,
                           thin = 50, warmup = 20000, refresh = 2000)
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                      init = 0.1, warmup = warm, iter = iter)
  set.seed(468)
  if (file.exists(file.path(cache_dir, sprintf("dat_%s.rds", code)))) {
    dat = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
  } else {
    dat = generate_datasets(gen, nsim)
    saveRDS(dat, file.path(cache_dir, sprintf("dat_%s.rds", code)))
  }
  res = compute_SBC(dat, 
        bck,
        cache_mode     = "results", 
        cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

set.seed(4682)

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). This suggests that this model performs well enough and only few simulated models exhibit issues. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r agg_sbc_checks2, fig.height=8}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.fab)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# set large values to a max
dvfakemat[dvfakemat > 2000] = 2000

# compute one histogram per simulated data-set 
binwidth = 20 
breaks = seq(0, max(dvfakemat, na.rm=T) + binwidth, binwidth) 
histmat = matrix(NA, ncol = length(dat), nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakemat[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Distribution of simulated discriminations", y = "", x = "") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean RTs (ms)", title = "Means of simulated RTs") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD RTs (ms)", title = "SDs of simulated RTs") +
  theme_bw()
p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, 
                top = text_grob("Prior predictive checks: reaction times", 
                face = "bold", size = 14))

```

Again, this all looks good. 

```{r agg_sbc_checks3, fig.height=12}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          as.numeric(
                            gsub(".*, (.+)\\).*", "\\1", 
                                 priors[priors$class == "b",]$prior))), 
                                          unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

Rank histogramms, sample ECDF, the relationship between the simulated true parameters and the posterior estimates as well as z-score and posterior contraction of our population-level predictors all are acceptable for this model as well. 

### Posterior predictive checks

As the next step, we fit the model to the data, check whether there are divergence or rhat issues, and then check whether the chains have converged.


```{r agg_postpc, fig.height=6, message=T}

# fit the final model
set.seed(6824)
m.fab = brm(f.fab,
            df.fab, prior = priors,
            family = shifted_lognormal,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_fab_final"
            )
rstan::check_hmc_diagnostics(m.fab$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.fab) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.fab)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no pathological behaviour with E-BFMI, no divergent samples and no rhat that is higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r agg_postpc2, fig.height=9}

# get posterior predictions
post.pred = posterior_predict(m.fab, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.fab, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means compared to the real values per group
p2 = ppc_stat_grouped(df.fab$rt.cor, post.pred, df.fab$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means compared to the real values per cue
p3 = ppc_stat_grouped(df.fab$rt.cor, post.pred, df.fab$cue) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, p3,
          nrow = 3, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: RTs", 
                face = "bold", size = 14))

```

This model fits our data much better.

### Inferences

Now that we are convinced that we can trust our model, we have a look at its estimate and use the hypothesis function to assess our hypotheses and perform explorative tests. 

```{r final, fig.height=6}

# print a summary
summary(m.fab)

# get the estimates and compute groups
df.m.fab = as_draws_df(m.fab) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    ASD       = b_Intercept + b_diagnosis2,
    ADHD      = b_Intercept + b_diagnosis1,
    BOTH      = b_Intercept + b_diagnosis3,
    COMP      = b_Intercept + b_COMP
    )

# plot the posterior distributions
df.m.fab %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP"       ~ "COMP",
      "b_cue1"       ~ "Face",
      "b_diagnosis1:cue1" ~ "Interaction: ADHD",
      "b_diagnosis2:cue1" ~ "Interaction: ASD",
      "b_diagnosis3:cue1" ~ "Interaction: ADHD+ASD"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>% 
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# H1a: FAB effect in COMP
h1a = hypothesis(m.fab, 
                 "0 < 2*(diagnosis1:cue1 + diagnosis2:cue1 + diagnosis3:cue1 - cue1)")
h1a

# H1b: ADHD slower than COMP
h1b = hypothesis(m.fab, 
                 "0 < 2*diagnosis1 + diagnosis2 + diagnosis3")
h1b

# H1c: ASD slower than COMP
h1c = hypothesis(m.fab, 
                 "0 < 2*diagnosis2 + diagnosis1 + diagnosis3")
h1c

# H1d: FAB in ASD decreased compared to COMP
h1d = hypothesis(m.fab, 
                 "0 < 4*diagnosis2:cue1 + 2*diagnosis1:cue1 + 2*diagnosis3:cue1")
h1d

# H1e: FAB in ADHD differs from FAB in COMP (undirected)
h1e = hypothesis(m.fab, 
                 "0 > 4*diagnosis1:cue1 + 2*diagnosis2:cue1 + 2*diagnosis3:cue1", 
                 alpha = 0.025)
h1e

# Exploration

# E1: FAB generally
e1 = hypothesis(m.fab, "2*cue1 < 0", alpha = 0.025)
e1

# E2: FAB effect in ADHD
e2 = hypothesis(m.fab, "0 < -2*cue1 - 2*diagnosis1:cue1", alpha = 0.025)
e2

# E3: FAB effect in ASD
e3 = hypothesis(m.fab, "0 < -2*cue1 - 2*diagnosis2:cue1", alpha = 0.025)
e3

# E4: FAB effect in ADHD+ASD
e4 = hypothesis(m.fab, "0 < -2*cue1 - 2*diagnosis3:cue1", alpha = 0.025)
e4

# E5: FAB in ADHD differs from FAB in ASD
e5 = hypothesis(m.fab, 
                "0 < -2*diagnosis1:cue1 + 2*diagnosis2:cue1", alpha = 0.025)
e5

# E6: FAB in ADHD differs from FAB in BOTH
e6 = hypothesis(m.fab, 
                "0 < -2*diagnosis1:cue1 + 2*diagnosis3:cue1", alpha = 0.025)
e6

# E7: FAB in ASD differs from FAB in BOTH
e7 = hypothesis(m.fab, 
                "0 > -2*diagnosis2:cue1 + 2*diagnosis3:cue1", alpha = 0.025)
e7

# E8: FAB in COMP differs from FAB in BOTH
e8 = hypothesis(m.fab, 
                "0 < 2*diagnosis1:cue1 + 2*diagnosis2:cue1 + 4*diagnosis3:cue1", 
                alpha = 0.025)
e8

# E9: face in COMP versus face in ADHD
e9  = hypothesis(m.fab, 
                "0 < 2*diagnosis1 + diagnosis2 + diagnosis3 +
                     2*diagnosis1:cue1 + diagnosis2:cue1 + diagnosis3:cue1", 
                alpha = 0.025)
e9

# E10: object in COMP versus object in ADHD
e10 = hypothesis(m.fab, 
                "0 < 2*diagnosis1 + diagnosis2 + diagnosis3 -
                     (2*diagnosis1:cue1 + diagnosis2:cue1 + diagnosis3:cue1)", 
                alpha = 0.025)
e10

# E11: E10 > E9
e11 = hypothesis(m.fab, 
                "2*diagnosis1 + diagnosis2 + diagnosis3 -
                 (2*diagnosis1:cue1 + diagnosis2:cue1 + diagnosis3:cue1) >
                2*diagnosis1 + diagnosis2 + diagnosis3 +
                 (2*diagnosis1:cue1 + diagnosis2:cue1 + diagnosis3:cue1)", 
                alpha = 0.025)
e11

# extract predicted differences in ms instead of log data
df.new = df.fab %>% 
  select(diagnosis, cue) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, cue, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.fab, summary = F, 
               newdata = df.new %>% select(diagnosis, cue), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

st(df.ms, 
   summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
            'pctile(x)[97.5]','max(x)'))

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    COMP = rowMeans(select(., matches("COMP_.*")), na.rm = T),
    ADHD = rowMeans(select(., matches("ADHD_.*")), na.rm = T),
    ASD  = rowMeans(select(., matches("ASD_.*")), na.rm = T),
    BOTH = rowMeans(select(., matches("BOTH_.*")), na.rm = T),
    FAB  = rowMeans(select(., matches(".*_object")), na.rm = T) -
      rowMeans(select(., matches(".*_face")), na.rm = T),
    FAB_COMP = COMP_object - COMP_face,
    FAB_ADHD = ADHD_object - ADHD_face,
    FAB_ASD  = ASD_object  - ASD_face,
    FAB_BOTH = BOTH_object - BOTH_face,
    h1b  = ADHD - COMP,
    h1c  = ASD - COMP,
    h1d  = FAB_COMP - FAB_ASD,
    h1e  = FAB_ADHD - FAB_COMP,
    BOTH_COMP = BOTH - COMP
  )

st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    ADHDvCOMP_face   = ADHD_face   - COMP_face,
    ADHDvCOMP_object = ADHD_object - COMP_object,
    diff_ADHDvCOMP   = ADHDvCOMP_object - ADHDvCOMP_face
  ) %>% select(face, object, FAB, ADHDvCOMP_face, ADHDvCOMP_object, diff_ADHDvCOMP),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

As hypothesised, both autistic adults and adults with ADHD exhibited increased overall reaction times compared with the COMP group (COMP - ADHD: *estimate* = `r round(h1b$hypothesis$Estimate,2)` [`r round(h1b$hypothesis$CI.Lower,2)`, `r round(h1b$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1b$hypothesis$Post.Prob*100,2)`%; COMP - ASD: *estimate* = `r round(h1c$hypothesis$Estimate,2)` [`r round(h1c$hypothesis$CI.Lower,2)`, `r round(h1c$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1c$hypothesis$Post.Prob*100,2)`%). The model predicts that participants in the comparison group react `r round(mean(df.ms$h1b), 2)`ms [`r round(ci(df.ms$h1b)$CI_low, 2)`, `r round(ci(df.ms$h1b)$CI_high, 2)`] faster than the participants in the ADHD group and `r round(mean(df.ms$h1c), 2)`ms [`r round(ci(df.ms$h1c)$CI_low, 2)`, `r round(ci(df.ms$h1c)$CI_high, 2)`] faster than autistic participants. Additionally, the model predicts that the participants in the comparison group react `r round(mean(df.ms$BOTH_COMP), 2)`ms [`r round(ci(df.ms$BOTH_COMP)$CI_low, 2)`, `r round(ci(df.ms$BOTH_COMP)$CI_high, 2)`] faster than adults in the ADHD+ASD group. 

Our Bayesian linear mixed model with the median of correct reaction times as the outcome and diagnostic status, cue (face or object) and their interaction confirmed a face attention bias in our comparison group: COMP participants reacted faster in response to targets appearing on the side of the face compared to targets appearing on the side of the object (*estimate* = `r round(h1a$hypothesis$Estimate,2)` [`r round(h1a$hypothesis$CI.Lower,2)`, `r round(h1a$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1a$hypothesis$Post.Prob*100,2)`%). FAB was not credibly decreased in ASD participants compared to COMP participants (*estimate* = `r round(h1d$hypothesis$Estimate,2)` [`r round(h1d$hypothesis$CI.Lower,2)`, `r round(h1d$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1d$hypothesis$Post.Prob*100,2)`%). However, FAB was credibly higher in the ADHD than the COMP group (*estimate* = `r round(h1e$hypothesis$Estimate,2)` [`r round(h1e$hypothesis$CI.Lower,2)`, `r round(h1e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1e$hypothesis$Post.Prob*100,2)`%). Specifically, predicted reaction times based on the model estimate a FAB of `r round(mean(df.ms$FAB_COMP), 2)`ms [`r round(ci(df.ms$FAB_COMP)$CI_low, 2)`, `r round(ci(df.ms$FAB_COMP)$CI_high, 2)`] in the COMP group, `r round(mean(df.ms$FAB_ADHD), 2)`ms [`r round(ci(df.ms$FAB_ADHD)$CI_low, 2)`, `r round(ci(df.ms$FAB_ADHD)$CI_high, 2)`] in the ADHD group, `r round(mean(df.ms$FAB_ASD), 2)`ms [`r round(ci(df.ms$FAB_ASD)$CI_low, 2)`, `r round(ci(df.ms$FAB_ASD)$CI_high, 2)`] in the ASD group as well as `r round(mean(df.ms$FAB_BOTH), 2)`ms [`r round(ci(df.ms$FAB_BOTH)$CI_low, 2)`, `r round(ci(df.ms$FAB_BOTH)$CI_high, 2)`] in the ADHD+ASD group. These estimates are reflected in our exploration of FAB over all groups (*estimate* = `r round(e1$hypothesis$Estimate,2)` [`r round(e1$hypothesis$CI.Lower,2)`, `r round(e1$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e1$hypothesis$Post.Prob*100,2)`%) as well as in the separate clinical groups with our model revealing a credible FAB effect in the ADHD (*estimate* = `r round(e2$hypothesis$Estimate,2)` [`r round(e2$hypothesis$CI.Lower,2)`, `r round(e2$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e2$hypothesis$Post.Prob*100,2)`%) but not the ASD (*estimate* = `r round(e3$hypothesis$Estimate,2)` [`r round(e3$hypothesis$CI.Lower,2)`, `r round(e3$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e3$hypothesis$Post.Prob*100,2)`%) and the ADHD+ASD group (*estimate* = `r round(e4$hypothesis$Estimate,2)` [`r round(e4$hypothesis$CI.Lower,2)`, `r round(e4$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e4$hypothesis$Post.Prob*100,2)`%). 


Exploration regarding comparison between ADHD and COMP on face and object separately: 

* Face: *estimate* = `r round(e9$hypothesis$Estimate,2)` [`r round(e9$hypothesis$CI.Lower,2)`, `r round(e9$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e9$hypothesis$Post.Prob*100,2)`%
* Object: *estimate* = `r round(e10$hypothesis$Estimate,2)` [`r round(e10$hypothesis$CI.Lower,2)`, `r round(e10$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e10$hypothesis$Post.Prob*100,2)`%
* Object(ADHD-COMP) > Face(ADHD-COMP): *estimate* = `r round(e11$hypothesis$Estimate,2)` [`r round(e11$hypothesis$CI.Lower,2)`, `r round(e11$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e11$hypothesis$Post.Prob*100,2)`%

## Plots

```{r plot, fig.height=6}

# overall median reaction times
df.fab %>% 
  group_by(subID, diagnosis, cue) %>%
  summarise(
    rt.cor = mean(rt.cor, na.rm = T)
    ) %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    Target    = recode(cue, "face" = "Face-cued", "object" = "Object-cued")
  ) %>% 
  ggplot(aes(diagnosis, rt.cor, fill = Target, colour = Target)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  ylim(0, 700) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "", 
       x = "", 
       y = "Reaction time (ms)") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

ggsave("Fig3_rts.tif", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

# Explorative analysis of RTs considering number of saccades

Since saccadic behaviour may have influenced reaction times, we rerun the model concerning reaction times with a separate predictor coding the number of saccades of this stimulus pair. 

```{r rt_sac, message=T, fig.height=6}

# merge behaviour and saccades together
df.sac.fab = merge(df.fab.full, 
                   df.sac %>% group_by(subID, diagnosis, trl) %>% summarise(n.sac = n()), 
                   all.x = T) %>%
  # compute median rt.cor
  group_by(subID, diagnosis, cue, stm) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T),
    n.sac  = sum(n.sac, na.rm = T)
  )

# set the contrasts
contrasts(df.sac.fab$cue) = contr.sum(2)
contrasts(df.sac.fab$cue)
contrasts(df.sac.fab$diagnosis) = contr.sum(4)
contrasts(df.sac.fab$diagnosis)

# run the model > more iterations due to some suboptimal rhats in the first try
set.seed(1357)
m.rtsac = brm(rt.cor ~ diagnosis * cue + n.sac + (cue | subID) + (cue * diagnosis | stm),
            df.sac.fab, prior = priors,
            family = shifted_lognormal,
            iter = iter*2, warmup = warm*2,
            backend = "cmdstanr", threads = threading(8), file = "m_fab_sac"
            )
rstan::check_hmc_diagnostics(m.rtsac$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.rtsac) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.rtsac)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))


```


```{r rt_sac_pp}

# get posterior predictions
post.pred = posterior_predict(m.rtsac, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.rtsac, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

df.sac.fab = df.sac.fab %>% drop_na()

# distributions of means compared to the real values per group
p2 = ppc_stat_grouped(df.sac.fab$rt.cor, post.pred, df.sac.fab$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: RTs of trials with saccade covariate", 
                face = "bold", size = 14))

```

```{r rt_sac_final}

# print a summary
summary(m.rtsac)

# H1a: FAB effect in COMP
h1a = hypothesis(m.rtsac, 
                 "0 < 2*(diagnosis1:cue1 + diagnosis2:cue1 + diagnosis3:cue1 - cue1)")
h1a

# H1b: ADHD slower than COMP
h1b = hypothesis(m.rtsac, 
                 "0 < 2*diagnosis1 + diagnosis2 + diagnosis3")
h1b

# H1c: ASD slower than COMP
h1c = hypothesis(m.rtsac, 
                 "0 < 2*diagnosis2 + diagnosis1 + diagnosis3")
h1c

# H1d: FAB in ASD decreased compared to COMP
h1d = hypothesis(m.rtsac, 
                 "0 < 4*diagnosis2:cue1 + 2*diagnosis1:cue1 + 2*diagnosis3:cue1")
h1d

# H1e: FAB in ADHD differs from FAB in COMP (undirected)
h1e = hypothesis(m.rtsac, 
                 "0 > 4*diagnosis1:cue1 + 2*diagnosis2:cue1 + 2*diagnosis3:cue1", 
                 alpha = 0.025)
h1e

```

This model confirmed the same hypotheses. 

# Explorative analysis of subject-specific FAB 

## Overall RT, RADS and ASRS

```{r que_fab, fig.height=4}

# merge with the questionnaire values
df.que = df.fab.full %>%
  group_by(subID, diagnosis, stm, cue, ASRS_total, RAADS_total, adhd.meds, gender) %>%
  # summarise the median reaction time for each stimulus pair
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  ) %>%
  pivot_wider(names_from = cue, values_from = rt.cor) %>%
  # calculate the fab purely based on reaction times
  mutate(
    fab     = object - face,
    overall = (object + face) / 2 
  ) %>% group_by(subID, diagnosis, ASRS_total, RAADS_total, adhd.meds, gender) %>%
  # calculate the mean FAB per person
  summarise(
    fab     = mean(fab),
    overall = mean(overall)
  ) %>% ungroup() %>%
  select(subID, diagnosis, overall, fab, ASRS_total, RAADS_total, adhd.meds, gender)

# check normal distributions
p1 = ggplot(df.que, aes(sample = ASRS_total)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
p2 = ggplot(df.que, aes(sample = RAADS_total)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
p3 = ggplot(df.que, aes(sample = fab)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
p4 = ggplot(df.que, aes(sample = overall)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
ggarrange(p1, p2, p3, p4,
          nrow = 2, ncol = 2, labels = "AUTO")

```

```{r que_fab_cor}

# do a Bayesian Spearman correlation: https://osf.io/j5wud
source("./helpers/rankBasedCommonFunctions.R")
source("./helpers/spearmanSampler.R")

# Default beta prior width is set to a = b = 1 for the sampler 
if (file.exists("rho_ASRS.rds")) {
  rhoSamples.asrs = readRDS("rho_ASRS.rds")
} else {
  set.seed(5468)
  rhoSamples.asrs = 
    spearmanGibbsSampler(xVals = df.que$ASRS_total,
                         yVals = df.que$fab, 
                         nSamples = 5e3)
  saveRDS(rhoSamples.asrs, file = "rho_ASRS.rds")
}
if (file.exists("rho_RADS.rds")) {
  rhoSamples.rads = readRDS("rho_RADS.rds")
} else {
  set.seed(5468)
  rhoSamples.rads = 
    spearmanGibbsSampler(xVals = df.que$RAADS_total,
                         yVals = df.que$fab, 
                         nSamples = 5e3)
  saveRDS(rhoSamples.rads, file = "rho_RADS.rds")
}
if (file.exists("rho_RT.rds")) {
  rhoSamples.rt = readRDS("rho_RT.rds")
} else {
  set.seed(5478)
  rhoSamples.rt = 
    spearmanGibbsSampler(xVals = df.que$overall,
                         yVals = df.que$fab, 
                         nSamples = 5e3)
  saveRDS(rhoSamples.rt, file = "rho_RT.rds")
}

# give the posterior samples for rho to the function below to compute BF01
asrs.bf = computeBayesFactorOneZero(rhoSamples.asrs$rhoSamples, 
                          whichTest = "Spearman",
                          priorParameter = 1)
rads.bf = computeBayesFactorOneZero(rhoSamples.rads$rhoSamples, 
                          whichTest = "Spearman",
                          priorParameter = 1)
RT.bf = computeBayesFactorOneZero(rhoSamples.rt$rhoSamples, 
                          whichTest = "Spearman",
                          priorParameter = 1)

```

Furthermore, Bayesian Spearman correlations revealed `r interpret_bf(asrs.bf)` associations between FAB and questionnaires assessing ASD (RADS: log(*BF*) = `r round(log(rads.bf),2)`) or ADHD (ASRS: log(*BF*) = `r round(log(asrs.bf),2)`). Last, we explored whether FAB was associated with overall reaction times to assess whether attenuated FAB leads to better or worse task performance. A Bayesian Spearman correlation revealed `r interpret_bf(RT.bf)` an association (log(*BF*) = `r round(log(RT.bf),2)`). 


```{r que_fab_plot}

# focus on the FAB effect
df.que %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    diagnosis = factor(diagnosis, levels = c("ADHD", "ADHD+ASD", "ASD", "COMP")),
    diagnosis_med = as.factor(case_when(
      adhd.meds & diagnosis == "ADHD" ~ "ADHD medicated",
      adhd.meds & diagnosis == "ADHD+ASD" ~ "ADHD medicated+ASD",
      diagnosis == "ADHD" ~ "ADHD unmedicated",
      diagnosis == "ADHD+ASD" ~ "ADHD unmedicated+ASD",
      T ~ diagnosis
    ))
  ) %>% 
  ggplot(aes(diagnosis, fab, fill = diagnosis_med, colour = diagnosis_med)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  geom_hline(yintercept = 0) +
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  labs(title = "Face attention bias", 
       x = "", 
       y = "Difference (ms)") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15),
        legend.title=element_blank())

ggsave("Fig4_FAB.tif", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

# focus on the FAB effect
df.que %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    diagnosis = factor(diagnosis, levels = c("ADHD", "ADHD+ASD", "ASD", "COMP"))
  ) %>% 
  ggplot(aes(diagnosis, fab, fill = diagnosis, colour = diagnosis)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  geom_hline(yintercept = 0) +
  scale_fill_manual(values = custom.col[c(3, 4, 5, 6)]) +
  scale_color_manual(values = custom.col[c(3, 4, 5, 6)]) +
  labs(title = "Face attention bias", 
       x = "", 
       y = "Difference (ms)") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15),
        legend.title=element_blank())

ggsave("Fig4_FAB_new.tif", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

# plot the associations
df.que %>% 
  pivot_longer(cols = c(ASRS_total, RAADS_total), names_to = "questionnaire") %>%
  ggplot(., aes(y = fab, x = value)) +
  geom_point(colour = c_mid_highlight, alpha = 0.75) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              colour = c_dark_highlight) +
  facet_grid(. ~ questionnaire, scale = "free_x") +
  theme_bw() 

```

## Gender

```{r fab_gen}

df.que %>% 
  mutate(fab = fab > 0) %>%
  group_by(diagnosis, gender, fab) %>% count()

aov.gen = anovaBF(fab ~ gender * diagnosis, data = df.que %>% filter(gender != "dan"))
aov.gen@bayesFactor

```

## FAB as percentage of total reaction time

Since ADHD participants took longer on both cues, we want to make sure that the effect of FAB is actually increased and not just larger because they take longer to response. 

```{r fab_perc}

df.fab.wide = df.fab %>% 
  pivot_wider(names_from = cue, values_from = rt.cor) %>%
  mutate(fab.perc = (object - face)*100/(object + face))

# set weakly informed priors
priors = c(
  prior(normal(0,   0.50), class = Intercept),
  prior(normal(1.0, 1.00), class = sd),
  prior(lkj(2),            class = cor),
  prior(normal(0,   1.00), class = b),
  prior(normal(0,  10.00), class = sigma)
)

m.fab.perc = brm(fab.perc ~ diagnosis + (1 | subID) + (diagnosis | stm) ,
                df.fab.wide, prior = priors,
                iter = iter, warmup = warm,
                backend = "cmdstanr", threads = threading(8),
                file = "m_fab_perc", 
                seed = 222
                )
rstan::check_hmc_diagnostics(m.fab.perc$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.fab.perc) >= 1.01, na.rm = T)

# print the summary
summary(m.fab.perc)

# FAB in ADHD > FAB in COMP
hypothesis(m.fab.perc, "diagnosis1 > - diagnosis1 - diagnosis2 - diagnosis3")

# print an overview
kable(df.fab.wide %>% group_by(diagnosis, subID) %>% 
        summarise(fab.perc = mean(fab.perc)) %>%
        group_by(diagnosis) %>% 
        summarise(
          fab.perc.avg = mean(fab.perc),
          fab.perc.se  = sd(fab.perc)/sqrt(n())
        ) %>% arrange(desc(fab.perc.avg)))

```

# Explorative analysis of errors

Last but not least, we are going to explore possible differences with regards to mean accuracies using a bernoulli distribution. 

Next, we are going to explore possible differences with regards to accuracy. We use a bernoulli distribution to model the threshold between correct and incorrect trials. We computed the SBC outside of this script in batches to avoid running out of memory. Then, we combined it and load the results in here. 

## Simulation-based calibration

```{r err_model, fig.height=12}

# figure out slopes for subject
kable(head(df.fab.full %>% count(subID, cue)))
kable(head(df.fab.full %>% count(stm, cue, diagnosis)))

code = "FAB_err"

# increase iterations a bit to improve rhats
iter = 4000
warm = 2000

# code accuracy to track errors
df.fab.full = df.fab.full %>%
  mutate(
    error = if_else(acc,0,1)
  )

# set the formula
f.err = brms::bf(error ~ diagnosis * cue + (cue | subID) + (diagnosis * cue | stm) )

# set weakly informed priors
priors = c(
  prior(normal(6.0,   1.00), class = Intercept),
  prior(normal(1.0,   0.50), class = sd),
  prior(lkj(2),  class = cor),
  # no specific expectations for the rest of the effects
  prior(normal(0,     1.00), class = b)
)

```

```{r err_checks1, fig.height=12, eval=F}

# check if the SBC already exists
if (file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) {
  # load in the resultsn of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
  dat        = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
} else {
  # perform SBC
  gen = SBC_generator_brms(f.err, data = df.fab.full, prior = priors, 
   thin = 50, warmup = 10000, refresh = 2000,
   generate_lp = TRUE, family = bernoulli, init = 0.1)
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
    warmup = warm, iter = iter)
  if (!file.exists(file.path(cache_dir, sprintf("dat_%s.rds", code)))) {
    dat = generate_datasets(gen, nsim)
    saveRDS(dat, file.path(cache_dir, sprintf("dat_%s.rds", code)))
  } else {
    dat = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
  }
  res = compute_SBC(dat, 
        bck,
        cache_mode     = "results", 
        cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

Looking at the rhats and divergent transitions shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05 and `r nrow(df.backend %>% filter(n_divergent > 0))` had divergent samples. Therefore, we continue with this model and plot the simulated values to perform prior predictive checks. 

```{r err_checks2, fig.height=20, eval=F}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.err)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# compute one histogram per simulated data-set 
options = c(0, 1)
histmat = matrix(NA, ncol = nrow(truePars), length(options)) 
for (i in 1:nrow(truePars)) {
  for (j in 1:length(options))
  {
    histmat[j,i] = sum(dvfakemat[,i] == options[j])
  }
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = c("error", "correct")
p0 = ggplot(data = quantmat, aes(x = x)) + 
  geom_bar(aes(y = p0.9), fill = c_light, stat = "identity") + 
  geom_bar(aes(y = p0.8), fill = c_light_highlight, stat = "identity") + 
  geom_bar(aes(y = p0.7), fill = c_mid, stat = "identity") + 
  geom_bar(aes(y = p0.6), fill = c_mid_highlight, stat = "identity") + 
  geom_bar(aes(y = p0.5), fill = c_dark, stat = "identity") + 
  labs(title = "Prior predictive distribution", y = "", x = "") +
  theme_bw()

# get simulation numbers with issues
check = merge(df.results %>% 
    group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T), max_rank = min(max_rank)) %>% 
    filter(rhat >= 1.05 | max_rank != max(max_rank)), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = 0.5) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

prior_sd = setNames(rep(1, length(unique(df.results.b$variable))), # all same SD 
                    unique(df.results.b$variable))

p4 = plot_contraction(df.results.b, prior_sd = prior_sd) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p0, p1, p2, p3, p4, 
              labels = "AUTO", ncol = 1, nrow = 5, 
              heights = c(1, 2, 2, 2, 2))
annotate_figure(p, 
                top = text_grob("Prior predictive checks and SBC", 
                face = "bold", size = 14))

```

Everything looks good with the wider priors, so we continue and run the model.

## Posterior predictive checks

As the next step, we fit the model, check whether there are divergence or rhat issues, and then check whether the chains have converged.

```{r err_postpc, fig.height=4, message=T}

# fit the final model
set.seed(1234)
m.err = brm(f.err,
            df.fab.full, prior = priors,
            iter = iter, warmup = warm,
            family = "bernoulli",
            backend = "cmdstanr", threads = threading(8),
            file = "m_err",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.err$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.err) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.err)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 4)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

Again, we use the function brms::pp_check() with `r nsim` draws to check whether the predicted data resembles the actual data as well as the ppc_stat_grouped function from the bayesplot package to check posterior fit for each diagnostic group separately. The model seems to be a good fit with the predicted data closely mirroring the real data. 

```{r err_postpc2, fig.height=6}

# get posterior predictions
post.pred = posterior_predict(m.err, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.err, ndraws = nsim, type = "bars") + 
  theme_bw() + theme(legend.position = "none") + labs(y = "")

# distributions of means compared to the real values per group
p2 = ppc_stat_grouped(df.fab.full$error, post.pred, df.fab.full$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2,
  nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: errors", 
                face = "bold", size = 14))

```

Our model fits the data very well.

## Inferences

Now that we are convinced that we can trust our model, we have a look at its estimate and use the hypothesis function to perform explorative tests.

```{r err_final, fig.height=9}

# print a summary
summary(m.err)

# plot the posterior distributions
as_draws_df(m.err) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP     = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
  ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = str_replace_all(coef, "cue1", "Face"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(c_light, c_dark)) + theme(legend.position = "none")

# COMP < ADHD
e1 = hypothesis(m.err, "0 < 2*diagnosis1 + diagnosis2 + diagnosis3", alpha = 0.025)
e1

# COMP < ASD
e2 = hypothesis(m.err, "0 < 2*diagnosis2 + diagnosis1 + diagnosis3", alpha = 0.025)
e2

# COMP < BOTH
e3 = hypothesis(m.err, "0 > 2*diagnosis3 + diagnosis1 + diagnosis2", alpha = 0.025)
e3

# explore differences between cues
e4 = hypothesis(m.err, "0 < 2*cue1", alpha = 0.025)
e4

# extract predicted differences
df.new = df.fab.full %>% 
  select(diagnosis, cue) %>% 
  mutate(
    condition = paste0(diagnosis, '_', cue)
  ) %>%
  distinct()
df.ms = as.data.frame(
  fitted(m.err, summary = F, 
               newdata = df.new, 
               re_formula = NA))
colnames(df.ms) = df.new$condition

st(df.ms, 
           summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
                    'pctile(x)[97.5]','max(x)'))

st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

Accuracies were generally high, with a total of `r round(100*mean(df.fab.full$error),2)`% inaccurate responses across diagnostic groups. The explorative analysis of the error rates revealed no credible differences between any of the diagnostic groups and no credible difference between cues (see supplementary materials). 

## Plots

```{r plot_err, fig.height=4}

# overall accuracies
df.fab.full %>% 
        group_by(subID, diagnosis, cue) %>% 
        summarise(error = 100*mean(error, na.rm = T)) %>% 
  ggplot(aes(diagnosis, error, fill = cue, colour = cue)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  ylim(0, 25) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Mean error rates", x = "", y = "percent") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```

# Summary table

```{r sum}

# get grand average of accuracies and reaction times
df.agg = rbind(
  df.fab.full %>%
    group_by(subID, diagnosis, cue) %>% 
    summarise(error = 100*mean(error, na.rm = T)) %>% 
    group_by(diagnosis, cue) %>% 
    summarise(mean = mean(error, na.rm = T), se = sd(error, na.rm = T)/sqrt(n())) %>%
    mutate(measure = "accuracy") %>%
    mutate(
        value = sprintf("%.2f ±%.2f", mean, se)
    )  %>% select(measure, diagnosis, cue, value) %>%
    pivot_wider(names_from = c(diagnosis, cue), values_from = value),
  df.fab.full %>%
    group_by(subID, diagnosis, cue) %>% 
    summarise(rt.cor = mean(rt.cor, na.rm = T)) %>% 
    group_by(diagnosis, cue) %>% 
    summarise(mean = mean(rt.cor, na.rm = T), se = sd(rt.cor, na.rm = T)/sqrt(n())) %>% 
    mutate(measure = "rt.cor")  %>%
    mutate(
        value = sprintf("%.0f ±%.0f", mean, se)
    )  %>% select(measure, diagnosis, cue, value) %>%
    pivot_wider(names_from = c(diagnosis, cue), values_from = value))

read_docx() %>%
    body_add_table(df.agg) %>%
    print(target = "FAB_tbl2.docx")

```

# Preparation for eye-tracking analysis

First, we reload the data. Second, we preprocess the latencies of the saccades and divide them into saccades elicited by the cues and saccades elicited by the target. To do so, we use the knowledge that latencies below 100ms are extremely unlikely and use the global minimum in the density function. 

```{r prep, fig.height=4}

# number of simulations
nsim = 250

# set the seed
set.seed(2468)

# load the data
load("FAB_data.RData")

# combine both behavioural datasets
df.fab = rbind(df.fab, df.exp)
df.fab$diagnosis = factor(df.fab$diagnosis, 
                          levels = c("ADHD", "ASD", "BOTH", "COMP"))

# compute subject specific FAB 
df.fab.agg = df.fab %>%
  group_by(subID, diagnosis, stm, cue) %>%
  # summarise the median reaction time for each stimulus pair
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  ) %>%
  pivot_wider(names_from = cue, values_from = rt.cor) %>%
  # calculate the fab purely based on reaction times
  mutate(
    fab = object - face
  ) %>% group_by(subID, diagnosis) %>%
  # calculate the mean FAB per person
  summarise(
    fab = mean(fab)
  ) %>% ungroup()

# remove participants without any data
df.sac = df.sac %>% filter(!is.na(lat)) %>% ungroup() %>%
  # remove unbelievably short and extremely long saccades
  filter(lat <= quantile(lat, probs = 0.99) &
           lat > 100) %>%
  # recode so that it is more consistent
  mutate(
    direction = if_else(dir_face, "face", "object")
  )

# divide into cue and target saccades
criticalpoints = function(density, threshold = 1){
  up   = sapply(1:threshold, function(n) c(density$y[-(seq(n))], rep(NA, n)))
  down = sapply(-1:-threshold, 
                function(n) c(rep(NA,abs(n)), 
                              density$y[-seq(length(density$y), 
                                             length(density$y) - abs(n) + 1)]))
  a    = cbind(density$y,up,down)
  minima = round(density$x[which(apply(a, 1, min) == a[,1])])
  maxima = round(density$x[which(apply(a, 1, max) == a[,1])])
  return(list(minima = minima, maxima = maxima))
}

points = criticalpoints(density(df.sac$lat))

# get the density of the latencies
dd = with(density(df.sac$lat), data.frame(x,y))

# find which point is the global minimum
lat.points = dd$y[points$minima]
idx = which.min(lat.points)

# print the latency
points$minima[idx]

# plot it all
ggplot(dd, aes(x = x, y = y)) + 
  geom_line() +
  geom_vline(xintercept = points$minima[idx], linetype=3) +
  geom_ribbon(data = subset(dd, x <= points$minima[idx]), 
              aes(ymax = y, fill = "cue-elicited"), ymin = 0, 
              colour = "black", alpha = .8) +
  geom_ribbon(data = subset(dd, x >= points$minima[idx]), 
              aes(ymax = y, fill = "target-elicited"), ymin = 0, 
              colour = "black", alpha = .8) +
scale_fill_manual(name = "test",
values = c("cue-elicited" = custom.col[7], "target-elicited" = custom.col[3]),
labels = c("Cue-elicited", "Target-elicited")) + 
  geom_vline(xintercept = 200) + 
  labs(title = "Classification of saccades", x = "Latency", y = "Density") +
  xlim(0, 800) +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15),
        legend.title=element_blank())

ggsave("Fig2_densLatency.tif", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

The graph above shows the density of the latencies with zero on the x-axis being the onset of the cue. After 200ms, the cue disappears and the target is presented on the screen (solid line). We can see that there is a minimum about 130ms after the target appears (dotted line). We can assume that saccades produced before this were in response to the cue (pink) and saccades after were in response to the target (green). Therefore, we divide the saccades accordingly. Then, we aggregate the data per subject and cue. Last, we set all predictors to sum contrasts.

```{r prepro}

# summarise overall saccade count based on direction: whole trial
df.cnt = df.sac %>% 
  group_by(subID, direction) %>% 
  summarise(
    n.sac = n()
  )

# add a zero if no saccades were produced
subID    = rep(as.character(unique(df.sac$subID)), 
               each = length(unique(df.sac$direction)))
direction = rep(as.character(unique(df.sac$direction)), 
               times = length(unique(df.sac$subID)))
df.cnt = merge(df.cnt, data.frame(subID, direction), all = T) %>%
  mutate(
    n.sac = if_else(is.na(n.sac), 0, n.sac)
  ) %>% 
  # merge with behavioural data
  merge(., df.fab.agg) %>%
  mutate_if(is.character, as.factor)

# code whether or not cue associated saccade occured and capture latencies
df.cue = merge(df.fab, 
               df.sac %>% filter(lat <= points$minima[idx] & sac_trl == 1), 
               all = T) %>%
  select(subID, diagnosis, trl, stm, cue, rt.cor, acc, direction, lat) %>%
  mutate(
    sac = if_else(is.na(direction),0,1)
  ) %>%
  mutate_if(is.character, as.factor)

# preprocess target latencies
df.lat = df.sac %>% 
  # only keep latencies associated with target
  filter(lat > points$minima[idx]) %>%
  # only keep the first target saccade latency of each trial
  group_by(subID, diagnosis, trl, cue) %>%
  filter(sac_trl == min(sac_trl)) %>%
  merge(., df.fab) %>%
  mutate_if(is.character, as.factor)

# set and print the contrasts
contrasts(df.lat$cue) = contr.sum(2)
contrasts(df.lat$cue)
contrasts(df.lat$diagnosis) = contr.sum(4)
contrasts(df.lat$diagnosis)
contrasts(df.cue$direction) = contr.sum(2)
contrasts(df.cue$direction)
contrasts(df.cue$diagnosis) = contr.sum(4)
contrasts(df.cue$diagnosis)
contrasts(df.cnt$direction) = contr.sum(2)
contrasts(df.cnt$direction)
contrasts(df.cnt$diagnosis) = contr.sum(4)
contrasts(df.cnt$diagnosis)

```

# Number of saccades towards face during trial

First, we are going to assess the saccades that are produced in the direction of the face throughout the full trial: cue and target presentation. Based on Entzmann et al. (2021), we hypothesised that COMP participants produce more saccades towards the face than towards the object cues during the trials. 

## Specify the model

Since we are counting the number of saccades, we use a poisson distribution for our model. We add an group-level intercept for each subject, and assess the influence of the predictors diagnostic status and whether the saccade was directed towards the side of the face or object as well as the interaction. We set our priors very wide because we do not have strong prior assumptions apart from people producing fewer saccades than there are trials. 

```{r model_cnt}

code = "CNT"

# set the formula
f.cnt = brms::bf(n.sac ~ diagnosis * direction + (1 | subID))

# set priors based on study design
priors = c(
  prior(normal(3,  1.5),    class = Intercept), 
  prior(normal(0,  1.0),    class = sd),
  prior(normal(0,  1.0),    class = b)
)

# set number of iterations and warmup for models
iter = 4500
warm = 1500

```

## Simulation-based calibration

``` {r sbc_cnt}

# check if the SBC already exists
if (file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
  dat        = readRDS(file.path(cache_dir, sprintf("dat_%s.rds", code)))
} else {
  # perform the SBC
  set.seed(2468)
  gen = SBC_generator_brms(f.cnt, data = df.cnt, prior = priors, 
   thin = 50, warmup = 10000, refresh = 2000,
   generate_lp = TRUE, family = poisson(), init = 0.1)
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
    warmup = warm, iter = iter)
  dat = generate_datasets(gen, nsim)
  saveRDS(dat, file.path(cache_dir, sprintf("dat_%s.rds", code)))
  res = compute_SBC(dat, 
        bck,
        cache_mode     = "results", 
        cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}
```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and only `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples. This suggests that this model performs well.

## Prior predictive checks

Next, we can plot the simulated values to perform prior predictive checks. 

```{r prior_checks_cnt, fig.height=8}

# get the true values
truePars = dat[["variables"]]

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.cnt)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

# set very large data points to a value of 432
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH > 432] = 432
# compute one histogram per simulated data-set 
breaks = seq(0, max(dvfakematH, na.rm=T), length.out = 100) 
binwidth = round(breaks[2] - breaks[1])
breaks = seq(0, max(dvfakematH, na.rm=T), binwidth) 
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "number of saccades") +
  theme_bw()

tmpM = apply(dvfakematH, 2, mean) # mean 
tmpSD = apply(dvfakematH, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean number of saccades", title = "Means of simulated data") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD number of saccades", title = "Standard deviations of simulated data") +
  theme_bw()

p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, top = text_grob("Prior predictive checks", face = "bold", size = 14))

```

Since our priors were set very wide, we do get wide prior predictive distributions. We accept this and continue with our checks. 

## Computational faithfulness and model sensitivity

```{r comp_check_cnt, fig.height=12}

# get simulation numbers with issues
des_rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = max(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank < des_rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters

df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id)) %>%
  ungroup() %>%
  mutate(
    max_rank = max(rank)
  )
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(
                            as.numeric(
                              gsub(".*, (.+)\\).*", "\\1", 
                                   priors[priors$class == "b",]$prior)),
                            length(unique(df.results.b$variable))-1)), 
                        unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

All of this looks good. Despite our wide priors, the contraction shows a bit of a distribution which increases our trust that the wide priors are appropriate.

## Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_cnt, message=T, fig.height=4}

# fit the model
set.seed(2468)
m.cnt = brm(f.cnt,
            df.cnt, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_cnt-face",
            family = "poisson", 
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.cnt$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.cnt) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.cnt)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_cnt, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.cnt, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.cnt, ndraws = nsim) + 
  theme_bw()

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.cnt$n.sac, post.pred, df.cnt$diagnosis) + 
  theme_bw()

p = ggarrange(p1, p2,
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: number of saccades towards face", 
                face = "bold", size = 14))

```

The predictions based on the model capture the data well. This further increases our trust in the model. 

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_cnt, fig.height=6}

# print a summary
summary(m.cnt)

# get the estimates and compute groups
df.m.cnt = as_draws_df(m.cnt) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    ASD       = b_Intercept + b_diagnosis2,
    ADHD      = b_Intercept + b_diagnosis1,
    BOTH      = b_Intercept + b_diagnosis3,
    COMP      = b_Intercept + b_COMP
    )

# plot the posterior distributions
df.m.cnt %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP"       ~ "COMP",
      "b_direction1" ~ "Face",
      "b_diagnosis1:direction1" ~ "Interaction: ADHD",
      "b_diagnosis2:direction1" ~ "Interaction: ASD",
      "b_diagnosis3:direction1" ~ "Interaction: ADHD+ASD"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# H2a: COMP: face > object
h2a = hypothesis(m.cnt, 
                 "0 < 2*(direction1 - diagnosis1:direction1 - 
                 diagnosis2:direction1 - diagnosis3:direction1)")
h2a

# explore general FAB
e1 = hypothesis(m.cnt, "0 < 2*direction1", 
                alpha = 0.025)
e1

# extract predicted differences
df.new = df.cnt %>% 
  select(diagnosis, direction) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, direction, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.cnt, summary = F, 
               newdata = df.new %>% select(diagnosis, direction), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

Our hypothesis regarding the number of saccades towards face or object cues was not confirmed: there was no credible difference in our COMP group (*estimate* = `r round(h2a$hypothesis$Estimate,2)` [`r round(h2a$hypothesis$CI.Lower,2)`, `r round(h2a$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2a$hypothesis$Post.Prob*100,2)`%). Exploration of FAB effect regardless of the group similarly indicates no FAB in the form of a larger number of saccades produced towards the faces over the whole trial (*estimate* = `r round(e1$hypothesis$Estimate,2)` [`r round(e1$hypothesis$CI.Lower,2)`, `r round(e1$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e1$hypothesis$Post.Prob*100,2)`%). 

## Plots

As a next step, we can now finally plot our data. 

```{r plot_cnt, fig.height=4}

# rain cloud plot
df.cnt %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    Direction = recode(direction, "face" = "Face", "object" = "Object")
  ) %>%
  ggplot(aes(diagnosis, n.sac, fill = Direction, colour = Direction)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Number of saccades", x = "", y = "Count") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

ggsave("Fig5_nrSac.tif", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

# Latencies of target-elicited saccades

Next, we focus on the latencies of the saccades that are produced during the presentation of the targets to assess whether cue type, diagnostic group or their interaction influence latencies. We hypothesised that ASD participants show an increased latency compared to COMP participants when producing saccades towards targets appearing at the location of a face. 

## Full model

We start with the model containing all latencies of saccades produced during the target presentation. We choose a shifted lognormal distribution because saccade latencies below 100ms are very unlikely. Additionally, latencies are determined based on the onset of the cue presentation (200ms). The SBC was run on three groups. 

### Setting up and assessing the model

```{r model_lat}

code = "LAT"

# set the formula
f.lat = brms::bf(lat ~ diagnosis * cue + (cue | subID) + (diagnosis * cue | stm))

# set weakly informative priors
priors = c(
  prior(normal(5,    0.75), class = Intercept),
  prior(normal(0,    0.25), class = sd),
  prior(normal(0,    0.25), class = b),
  prior(normal(0.5,  0.50), class = sigma),
  prior(normal(350, 50.00), class = ndt),  # threshold between target and cue saccades
  prior(lkj(2),             class = cor)
)

# set number of iterations and warmup for models
iter = 3000
warm = 1000

```

``` {r sbc_lat}

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file = file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # create the data and the results
  set.seed(2468)
  gen = SBC_generator_brms(f.lat, data = df.lat, prior = priors, 
                           family = "shifted_lognormal",
                           thin =  50, warmup = 10000, refresh = 2000,
                           generate_lp = TRUE)
  dat = generate_datasets(gen, nsim) 
  saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  backend = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                            warmup = 1000, iter = 3000)
  results = compute_SBC(dat, backend,
                        cache_mode     = "results", 
                        cache_location = file.path(cache_dir, paste0("res_", code)))
  saveRDS(results$stats, 
          file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(results$backend_diagnostics, 
          file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05 and only `r nrow(df.backend %>% filter(n_divergent > 0))` model had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). This suggests that this model performs well.

### Prior predictive checks

Next, we can plot the simulated values to perform prior predictive checks. 

```{r prior_checks_lat, fig.height=8}

# get the true values
truePars = dat[["variables"]]

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.lat)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

# set very large data points to a value of 1500
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH < 0]    = 0
dvfakematH[dvfakematH > 1500] = 1500
# compute one histogram per simulated data-set 
breaks = seq(0, 1500, length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "latency of saccades") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean latency of saccades", title = "Means of simulated data") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD latency of saccades", title = "Standard deviations of simulated data") +
  theme_bw()

p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, 
                top = text_grob("Prior predictive checks: latency", 
                face = "bold", size = 14))

```

First, we assess whether the simulated values fit our expectations of the distribution of the data. Previous literature has found that saccade latencies are around 200ms with few saccades being produced faster than 100ms. If we add 200ms from the cue presentation, this means we expect most latencies to be above 300ms and centered around 400ms. Our simulated datasets seem to capture this well. 

### Computational faithfulness and model sensitivity

```{r comp_check_lat, fig.height=12}

# get simulation numbers with issues
des_rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = max(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank < des_rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters

df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id)) %>%
  ungroup() %>%
  mutate(
    max_rank = max(rank)
  )
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(
                            as.numeric(
                              gsub(".*, (.+)\\).*", "\\1", 
                                   priors[priors$class == "b",]$prior)),
                            length(unique(df.results.b$variable))-1)), 
                        unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

All looks acceptable here.  

### Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_lat, fig.height=4, message=T}

# fit the maximal model
set.seed(2587)
m.lat = brm(f.lat,
            df.lat, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_lat",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.lat$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.lat) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.lat)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The model does not have any divergent transitions nor high rhats. The trace plots also look good, therefore, we move on to the posterior predictive checks. 

```{r postpc2_lat, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.lat, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.lat, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.lat$lat, post.pred, df.lat$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: latency", 
                face = "bold", size = 14))

```

The simulated data based on the model does not fit our data very well: it is wider and seems to underestimate latencies for COMP while overestimating for ADHD and ASD with the dark blue line showing the mean of the actual dataset and the light blue bars showing the distribution of the predicted data.

## Aggregated model

Since we want to base our inferences on the estimates, we go back to the drawing board and aggregate our data to see whether this resolves these issues. 

### Setting up and assessing the model

```{r model_lat_agg}

code = "LAT_agg"

# aggregate the data
df.lat.agg = df.lat %>% 
  group_by(subID, cue, diagnosis) %>% 
  summarise(lat = median(lat, na.rm = T))

# set the formula
f.lat = brms::bf(lat ~ diagnosis * cue + (1 | subID) )

# set weakly informative priors
priors = priors %>% filter(class != "cor")

# set number of iterations and warmup for models
iter = 3000
warm = 1000

```

Again, we ran the SBC based on the three original, preregistered groups. 

``` {r sbc_lat_agg}

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file = file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # create the data and the results
  set.seed(2468)
  gen = SBC_generator_brms(f.lat, data = df.lat.agg, prior = priors, 
                           family = "shifted_lognormal",
                           thin =  50, warmup = 10000, refresh = 2000,
                           generate_lp = TRUE)
  dat = generate_datasets(gen, nsim) 
  saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  backend = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                            warmup = 1000, iter = 3000)
  results = compute_SBC(dat, backend,
                        cache_mode     = "results", 
                        cache_location = file.path(cache_dir, paste0("res_", code)))
  saveRDS(results$stats, 
          file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(results$backend_diagnostics, 
          file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, but `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). This is something to look out for in the final model. 

### Prior predictive checks

Next, we can plot the simulated values to perform prior predictive checks. 

```{r prior_checks_lat_agg, fig.height=8}

# get the true values
truePars = dat[["variables"]]

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.lat)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

# set very large data points to a value of 1500
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH < 0]    = 0
dvfakematH[dvfakematH > 1500] = 1500
# compute one histogram per simulated data-set 
breaks = seq(0, 1500, length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "latency of saccades") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean latency of saccades", title = "Means of simulated data") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD latency of saccades", title = "Standard deviations of simulated data") +
  theme_bw()

p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, 
                top = text_grob("Prior predictive checks: latency", 
                face = "bold", size = 14))

```

Again, our simulated datasets seem to capture well what we know about saccade latencies.  

### Computational faithfulness and model sensitivity

```{r comp_check_lat_agg, fig.height=12}

# get simulation numbers with issues
des_rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = max(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank < des_rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters

df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id)) %>%
  ungroup() %>%
  mutate(
    max_rank = max(rank)
  )
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(
                            as.numeric(
                              gsub(".*, (.+)\\).*", "\\1", 
                                   priors[priors$class == "b",]$prior)),
                            length(unique(df.results.b$variable))-1)), 
                        unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

The intercept looks slightly off here, the model could have a slight tendency to underestimate it.

### Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_lat_agg, fig.height=4, message=T}

# fit the maximal model
set.seed(7799)
m.lat = brm(f.lat,
            df.lat.agg, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_lat_agg",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.lat$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.lat) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.lat)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The final model does not exhibit any divergence issues or suboptimal rhats. 

```{r postpc2_lat_agg, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.lat, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.lat, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.lat.agg$lat, post.pred, df.lat.agg$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: latency", 
                face = "bold", size = 14))

```

This looks much better with the simulated data based on the model capturing our actual data well. 

### Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_lat, fig.height=4}

# print a summary
summary(m.lat)

# plot the posterior distributions
as_draws_df(m.lat) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
    ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_cue1" ~ "Face",
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP" ~ "COMP",
      "b_diagnosis1:cue1" ~ "Interaction: ADHD x cue",
      "b_diagnosis2:cue1" ~ "Interaction: ASD x cue",
      "b_diagnosis3:cue1" ~ "Interaction: ADHD+ASD x cue"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# H2b: ASD(face) > COMP(face)
h2b = hypothesis(m.lat, 
                 "0 < diagnosis1 + diagnosis3 + 2*diagnosis2 +
                 diagnosis1:cue1 + diagnosis3:cue1 + 2*diagnosis2:cue1")
h2b

# explore: overall faster towards face-cued targets
e = hypothesis(m.lat, "0 > 2*cue1", alpha = 0.025)
e

# extract predicted differences 
df.new = df.lat %>% 
  select(diagnosis, cue) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, cue, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.lat, summary = F, 
               newdata = df.new %>% select(diagnosis, cue), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    e  = rowMeans(select(., matches(".*_object")), na.rm = T) - 
      rowMeans(select(., matches(".*_face")), na.rm = T)
  )

st(df.ms %>% 
  mutate(
    # get the face and object latencies -200, so they start with target onset
    face   = rowMeans(select(., matches(".*_face")), na.rm = T) - 200,
    object = rowMeans(select(., matches(".*_object")), na.rm = T) - 200,
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

Our hypothesis that target-elicited saccades towards the faces have a longer latency in ASD than COMP adults was not confirmed by the data (*estimate* = `r round(h2b$hypothesis$Estimate,2)` [`r round(h2b$hypothesis$CI.Lower,2)`, `r round(h2b$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2b$hypothesis$Post.Prob*100,2)`%). Our exploration revealed that that latencies of target-elicited saccades where faster in response to face-cued compared to object-cued targets regardless of the diagnostic group (*estimate* = `r round(e$hypothesis$Estimate,2)` [`r round(e$hypothesis$CI.Lower,2)`, `r round(e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e$hypothesis$Post.Prob*100,2)`%). Specifically, the model predicted a `r round(mean(df.ms$e), 2)`ms [`r round(ci(df.ms$e)$CI_low, 2)`, `r round(ci(df.ms$e)$CI_high, 2)`] shorter latency of for saccades produced towards a face-cued compared to an object-cued target. 

### Plots

```{r plot_lat, fig.height=4}

# rain cloud plot for the 
df.lat.agg %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD"),
    Target    = recode(cue, "face" = "Face-cued", "object" = "Object-cued")
  ) %>% 
  ggplot(aes(diagnosis, lat, fill = Target, colour = Target)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Latencies of target-elicited saccades", x = "", y = "Latency (ms)") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

ggsave("Fig6_latSac.tif", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

# Correlation with reaction times: number of saccades

Last, we hypothesised that the FAB effect on reaction times may be associated with saccades produced towards the face. To investigate this, we use a Bayesian Spearman correlation as both FAB effect and number of saccades are not normally distributed. 

```{r cor_n, fig.height=4}

# only keep saccades towards faces
df.diff = df.cnt %>% filter(direction == "face")

# check the distribution plot > not normally distributed
p1 = ggplot(df.diff, aes(sample = n.sac)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
p2 = ggplot(df.diff, aes(sample = fab)) + 
  stat_qq(alpha = 0.75, colour = c_mid_highlight) + 
  stat_qq_line() + 
  theme_bw()
ggarrange(p1, p2, 
          nrow = 1, ncol = 2, labels = "AUTO")

# do a Bayesian Spearman correlation: https://osf.io/j5wud
source("./helpers/rankBasedCommonFunctions.R")
source("./helpers/spearmanSampler.R")

# Default beta prior width is set to a = b = 1 for the sampler 
if (file.exists("rho_CNT.rds")) {
  rhoSamples.cnt = readRDS("rho_CNT.rds")
} else {
  set.seed(1597)
  rhoSamples.cnt = 
    spearmanGibbsSampler(xVals = df.diff$n.sac,
                         yVals = df.diff$fab, 
                         nSamples = 5e3)
  saveRDS(rhoSamples.cnt, file = "rho_CNT.rds")
}

# give the posterior samples for rho to the function below to compute BF01
cor.bf = computeBayesFactorOneZero(rhoSamples.cnt$rhoSamples, 
                          whichTest = "Spearman",
                          priorParameter = 1)

# visualise it
ggplot(data = df.diff, aes(x = n.sac, y = fab)) +
  geom_point(colour = c_mid_highlight) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              geom = "smooth", colour = c_dark_highlight) +
  theme_bw() 

```

Furthermore, we assessed the relationship between FAB and number of saccades produced towards the face on the participant level (see supplementary materials S2.4). We used a Bayesian Spearman correlation due to both values not being normally distributed. This model revealed no association between number of saccades and face attention bias, in fact there was `r interpret_bf(cor.bf)` an association between the number of saccades and face attention bias (log(*BF*) = `r round(log(cor.bf), 2)`).

# Exploration: cue-elicited saccades

Additionally to our hypotheses, we also explored any effects of diagnostic status, cue type and their interaction on whether a saccade was produced during the presentation of the cues to assess the findings of increased saccade frequency towards faces by Pereira et al. (2020)in our data. Since these are again count data, we will use a Poisson and then compare the overall descriptives and effects to Pereira and colleague's results. Pereira found about 6% of trials contained cue-elicited saccades which translates to an intercept of 3. Therefore, we can use the same priors and SBC as in our Poisson investigating numbers of saccades in general. 

```{r model_cnt_cue, fig.height=12}

# aggregate to counts
df.cnt.cue = df.cue %>% 
  group_by(subID, diagnosis) %>%
  summarise(
    face   = sum(direction == "face", na.rm = T),
    object = sum(direction == "object", na.rm = T)
  ) %>%
  pivot_longer(cols = c(face, object), names_to = "direction", values_to = "n.sac") %>%
  mutate_if(is.character, as.factor)

# aggregate for descriptives over both conditions
df.cnt.cue.agg = df.cnt.cue %>% group_by(subID, diagnosis) %>%
  summarise(n.sac = sum(n.sac))

# set the contrasts
contrasts(df.cnt.cue$direction) = contr.sum(2)
contrasts(df.cnt.cue$direction)
contrasts(df.cnt.cue$diagnosis) = contr.sum(4)
contrasts(df.cnt.cue$diagnosis)

# set the same formula
f.cnt = brms::bf(n.sac ~ diagnosis * direction + (1 | subID))

# set priors based on study design
priors = c(
  prior(normal(3,  1.5),    class = Intercept), 
  prior(normal(0,  1.0),    class = sd),
  prior(normal(0,  1.0),    class = b)
)

# set number of iterations and warmup for models
iter = 4500
warm = 1500

```

## Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_cnt_cue, message=T, fig.height=4}

# fit the model
set.seed(4682)
m.cnt = brm(f.cnt,
            df.cnt.cue, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_cnt-cue",
            family = "poisson", 
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.cnt$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.cnt) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.cnt)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_cnt_cue, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.cnt, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.cnt, ndraws = nsim) + 
  theme_bw()

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.cnt.cue$n.sac, post.pred, df.cnt.cue$diagnosis) + 
  theme_bw()

p = ggarrange(p1, p2,
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: number of cue-elicited saccades", 
                face = "bold", size = 14))

```

The predictions based on the model capture the data well. This further increases our trust in the model. 

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_cnt_cue, fig.height=6}

# print a summary
summary(m.cnt)

# get the estimates and compute groups
df.m.cnt = as_draws_df(m.cnt) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    ASD       = b_Intercept + b_diagnosis2,
    ADHD      = b_Intercept + b_diagnosis1,
    BOTH      = b_Intercept + b_diagnosis3,
    COMP      = b_Intercept + b_COMP
    )

# plot the posterior distributions
df.m.cnt %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP"       ~ "COMP",
      "b_direction1" ~ "Face",
      "b_diagnosis1:direction1" ~ "Interaction: ADHD",
      "b_diagnosis2:direction1" ~ "Interaction: ASD",
      "b_diagnosis3:direction1" ~ "Interaction: ADHD+ASD"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# face > object
e = hypothesis(m.cnt, "0 < 2*(direction1)", 
                alpha = 0.025)
e


# extract predicted differences
df.new = df.cnt.cue %>% ungroup() %>%
  select(diagnosis, direction) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, direction, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.cnt, summary = F, 
               newdata = df.new %>% select(diagnosis, direction), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = object - face
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

On average, participants produced cue-elicited saccades on `r round(100*mean(df.cnt.cue.agg$n.sac)/432, 2)`% +- `r round(sd(100*df.cnt.cue.agg$n.sac/432)/sqrt(nrow(df.cnt.cue.agg)), 2)` of the trials. However, the range was very wide, with some participants producing none and others producing them on `r round(100*max(df.cnt.cue.agg$n.sac)/432, 2)`% of the trials. Regardless of group, credibly more cue-elicited saccades were produced towards face compared to object cues (*estimate* = `r round(e$hypothesis$Estimate,2)` [`r round(e$hypothesis$CI.Lower,2)`, `r round(e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e$hypothesis$Post.Prob*100,2)`%).

## Plots

As a last step, we can plot our data. 

```{r plot_cnt_cue, fig.height=4}

# rain cloud plot
df.cnt %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD")
  ) %>%
  ggplot(aes(diagnosis, n.sac, fill = direction, colour = direction)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Number of cue-elicited saccades", x = "", y = "n") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```

# Exploration: Latencies of cue-elicited saccades

## Model

We assume that the SBC for the hypothesis-guided latency analysis holds for this. We only need to slightly adjust the prior for the shift. This analysis only includes participants who performed cue-elicited saccades. 

```{r model_lat_cue}

# preprocess cue latencies
df.lat.cue = df.cue %>%
  filter(!is.na(direction)) %>%
  group_by(subID, direction, diagnosis) %>% 
  summarise(lat = median(lat, na.rm = T)) %>%
  mutate_if(is.character, as.factor) 

# set the formula
f.lat = brms::bf(lat ~ diagnosis * direction + (1 | subID) )

# set weakly informative priors
priors = c(
  prior(normal(5,    0.75), class = Intercept),
  prior(normal(0,    0.25), class = sd),
  prior(normal(0,    0.25), class = b),
  prior(normal(0.5,  0.50), class = sigma),
  prior(normal(150, 50.00), class = ndt)  # this is the only prior that differs
)

# set number of iterations and warmup for models
iter = 3000
warm = 1000

# set the contrasts
contrasts(df.lat.cue$direction) = contr.sum(2)
contrasts(df.lat.cue$direction)
contrasts(df.lat.cue$diagnosis) = contr.sum(4)
contrasts(df.lat.cue$diagnosis)

```

```{r postpc_lat_cue, fig.height=4, message=T}

# fit the maximal model
set.seed(7799)
m.lat = brm(f.lat,
            df.lat.cue, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_lat-cue_agg",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.lat$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.lat) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.lat)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The final model does not exhibit any divergence issues or suboptimal rhats. 

```{r postpc2_lat_cue, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.lat, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.lat, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.lat.cue$lat, post.pred, df.lat.cue$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: latency cues", 
                face = "bold", size = 14))

```

This looks good.

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_lat_cue, fig.height=4}

# print a summary
summary(m.lat)

# plot the posterior distributions
as_draws_df(m.lat) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
    ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = case_match(coef,
      "b_direction1" ~ "Face",
      "b_diagnosis1" ~ "ADHD",
      "b_diagnosis2" ~ "ASD",
      "b_diagnosis3" ~ "ADHD+ASD",
      "b_COMP" ~ "COMP",
      "b_diagnosis1:direction1" ~ "Interaction: ADHD x direction",
      "b_diagnosis2:direction1" ~ "Interaction: ASD x direction",
      "b_diagnosis3:direction1" ~ "Interaction: ADHD+ASD x direction"
    ),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

# explore: faster towards faces
e = hypothesis(m.lat, "0 > 2*direction1", alpha = 0.025)
e

# extract predicted differences 
df.new = df.lat.cue %>% ungroup() %>% 
  select(diagnosis, direction) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, direction, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.lat, summary = F, 
               newdata = df.new %>% select(diagnosis, direction), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

st(df.ms,
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]',
           'pctile(x)[97.5]','max(x)'))

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    e  = rowMeans(select(., matches(".*_object")), na.rm = T) - 
      rowMeans(select(., matches(".*_face")), na.rm = T)
  )

st(df.ms %>% 
  mutate(
    face   = rowMeans(select(., matches(".*_face")), na.rm = T),
    object = rowMeans(select(., matches(".*_object")), na.rm = T),
    FAB    = e
  ) %>% select(face, object, FAB),
  summ = c('mean(x)','sd(x)','min(x)','pctile(x)[2.5]','pctile(x)[97.5]','max(x)'))

```

A similar effect was found when exploring the latencies of cue-induced saccade, with participants producing saccades towards face cues faster than saccades towards object cues, independent of group (*estimate* = `r round(e$hypothesis$Estimate,2)` [`r round(e$hypothesis$CI.Lower,2)`, `r round(e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e$hypothesis$Post.Prob*100,2)`%). This model predicted that if a cue-induced saccade was produced towards the face, the latency was `r round(mean(df.ms$e), 2)`ms [`r round(ci(df.ms$e)$CI_low, 2)`, `r round(ci(df.ms$e)$CI_high, 2)`] shorter than if a saccade was produced towards the object cue.  

## Plots

```{r plot_lat_cue, fig.height=4}

# rain cloud plot for the 
df.lat.cue %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD")
  ) %>% 
  ggplot(aes(diagnosis, lat, fill = direction, colour = direction)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Latencies of cue-elicited saccades", x = "", y = "ms") +
  theme_bw() + 
  ylim(0, 325) +
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```

# Exploration: Dwell times starting within cue-elicited saccade window

## Model

We assume that the SBC from the reaction time lognormal model holds here, although we slightly widen the Intercept because we have less prior knowledge.

```{r model_fix}

# read in the preprocessed data
df.fix = readRDS("FAB_ET_fix.rds")

# set the formula
f.fix = brms::bf(duration ~ diagnosis * ROI * onTar + 
                   (ROI * onTar | subID) + (ROI * onTar | subID))

# set weakly informative priors
priors = c(
  # general priors based on SBV
  prior(normal(6, 0.6),  class = Intercept),
  prior(normal(0, 0.5),  class = sigma),
  prior(normal(0, 0.1),  class = sd),
  prior(lkj(2),          class = cor),
  prior(normal(0,     0.04), class = b),
  # shift
  prior(normal(200,   100), class = ndt)
)

# set number of iterations and warmup for models
iter = 3000
warm = 1000

# set the contrasts
contrasts(df.fix$ROI) = contr.sum(2)
contrasts(df.fix$ROI)
contrasts(df.fix$onTar) = contr.sum(2)
contrasts(df.fix$onTar)
contrasts(df.fix$diagnosis) = contr.sum(4)
contrasts(df.fix$diagnosis)

```

```{r postpc_fix, fig.height=9, message=T}

# fit the maximal model
set.seed(5599)
m.fix = brm(f.fix,
            df.fix, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            family = "shifted_lognormal",
            file = "m_fix",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.fix$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.fix) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.fix)
mcmc_trace(post.draws, regex_pars = "^b_", 
           facet_args = list(ncol = 3)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

The final model does not exhibit any divergence issues or suboptimal rhats. 

```{r postpc_fix2, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.fix, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.fix, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.fix$duration, post.pred, df.fix$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: dwell times", 
                face = "bold", size = 14))

```

This looks good.

## Inferences

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_fix, fig.height=6}

# print a summary
summary(m.fix)

# plot the posterior distributions
as_draws_df(m.fix) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP    = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3
    ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  filter(coef != "b_Intercept") %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "ROI1", "FaceROI"),
    coef = str_replace_all(coef, "onTar1", "notTarget"),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

```

## Plots

```{r plot_fix, fig.height=4}

# rain cloud plot for the 
df.fix %>%
  group_by(subID, diagnosis, ROI, onTar) %>%
  summarise(
    duration = median(duration, na.rm = T)
  ) %>%
  mutate(
    diagnosis = recode(diagnosis, "BOTH" = "ADHD+ASD")
  ) %>% 
  ggplot(aes(diagnosis, duration, fill = ROI, colour = ROI)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show_guide = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col2) +
  scale_color_manual(values = custom.col2) +
  labs(title = "Dwell times starting during cue-elicited saccades", x = "", y = "") +
  theme_bw() + 
  facet_wrap(. ~ onTar) +
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

```
